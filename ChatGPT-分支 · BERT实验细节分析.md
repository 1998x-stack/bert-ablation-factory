# åˆ†æ”¯ Â· BERTå®éªŒç»†èŠ‚åˆ†æ

**User:** Anonymous (harbertparker90526@outlook.com)  
**Created:** 9/17/2025 10:23:53  
**Updated:** 9/17/2025 23:19:40  
**Exported:** 9/17/2025 23:20:05  
**Link:** [https://chatgpt.com/g/g-p-68c390f088dc81918596271e1836757f-deepsearch/c/68ca1bb8-ee68-8326-970f-895b9416fda2](https://chatgpt.com/g/g-p-68c390f088dc81918596271e1836757f-deepsearch/c/68ca1bb8-ee68-8326-970f-895b9416fda2)  

## Prompt:
2025-09-17 10:23:45

03-BERT.md

æ–‡ä»¶

03-BERT.pdf

PDF

æå…¶è¯¦ç»†åˆ†æPDF/MDä¸­å®éªŒç»†èŠ‚ï¼Œablationçš„ç»†èŠ‚ï¼Œæ•°æ®é›†ï¼Œæ¨¡å‹å‚æ•°ï¼Œè®­ç»ƒå‚æ•°ç­‰ç­‰ï¼š

å·²æ€è€ƒ 5m 45s

æ­£åœ¨è¯»å–æ–‡æ¡£

æ­£åœ¨è¯»å–æ–‡æ¡£

æˆ‘ä»¬ä»03-BERT.pdfè·å–äº†å‡ ä¸ªç›¸å…³çš„æœç´¢ç»“æœï¼Œé‡Œé¢æœ‰ä¸€äº›ç»†èŠ‚ã€‚è¿˜éœ€è¦æŸ¥æ‰¾æ•°æ®é›†è¯¦æƒ…ï¼Œåƒæ˜¯é¢„è®­ç»ƒè¯­æ–™åº“ï¼ˆåŒ…æ‹¬800Må•è¯çš„BooksCorpuså’Œ2500Må•è¯çš„Wikipediaï¼Œæ€»è®¡3.3Bå•è¯ï¼‰ã€‚è¿™äº›ç»†èŠ‚å¤§æ¦‚å‡ºç°åœ¨é™„å½•A.2éƒ¨åˆ†ã€‚æˆ‘ä»¬åº”è¯¥ç»§ç»­æœç´¢â€œBooksCorpus Wikipediaâ€ï¼Œè¿›è¡Œæ›´å¤šæŸ¥è¯¢ã€‚

æ­£åœ¨è¯»å–æ–‡æ¡£

æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥æ„å»ºè¯¦ç»†çš„ä¸­æ–‡åˆ†æã€‚ç”¨æˆ·å¸Œæœ›å¾—åˆ°å…³äºå®éªŒç»†èŠ‚ã€æ¶ˆèå®éªŒã€æ•°æ®é›†ã€æ¨¡å‹å‚æ•°å’Œè®­ç»ƒå‚æ•°çš„æä¸ºè¯¦ç»†çš„åˆ†æã€‚åˆ†æç»“æ„å¦‚ä¸‹ï¼š

*   æ‘˜è¦/æ€»è§ˆ
*   é¢„è®­ç»ƒæ•°æ®é›†ï¼šBooksCorpus + è‹±æ–‡Wikipediaï¼›3.3Bå•è¯ï¼›æ ·æœ¬æ€ä¹ˆé€‰å–ï¼›äºŒå¥è¯å¯¹ï¼›50% IsNextã€NotNextï¼›åºåˆ—é•¿åº¦<=512ï¼›90%ä½¿ç”¨128ï¼Œ10%ä½¿ç”¨512ï¼›WordPiece 30kï¼ˆå°šéœ€ç¡®è®¤ä½ç½®ï¼Œä½†å·²ç»çŸ¥é“ï¼‰ã€‚

æ­£åœ¨è¯»å–æ–‡æ¡£

åœ¨03-BERT.pdfä¸­ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†é¢„è®­ç»ƒæ•°æ®çš„ç»†èŠ‚ï¼ˆL70-L80ï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œéœ€è¦æŸ¥æ‰¾å…³äºé¢„è®­ç»ƒçš„è®­ç»ƒè¶…å‚æ•°ã€‚è¿™äº›åº”è¯¥åœ¨é™„å½•A.3æˆ–A.4ä¸­ï¼ŒåŒ…æ‹¬ä¼˜åŒ–æ–¹æ³•ï¼ˆAdamã€å­¦ä¹ ç‡1e-4ç­‰ï¼‰ï¼Œä»¥åŠè®­ç»ƒæ­¥éª¤ã€æ‰¹å¤„ç†ã€åºåˆ—é•¿åº¦ç­‰ã€‚æˆ‘ä»¬æ¥ä¸‹æ¥å¯ä»¥åœ¨æ–‡æ¡£ä¸­æœç´¢ä¸€äº›å…³é”®å­—ï¼Œå¦‚â€œAdamâ€ï¼Œâ€œwarmupâ€ï¼Œâ€œbatch size 256â€ï¼Œâ€œ512â€ï¼Œâ€œ90%â€ç­‰ã€‚

æ­£åœ¨è¯»å–æ–‡æ¡£

æˆ‘ä»¬å·²ç»æ”¶é›†äº†æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼Œä½†è¿˜éœ€è¦æ‰¾åˆ°SQuADçš„å¾®è°ƒç»†èŠ‚ã€‚é™„å½•Bä¸­åº”è¯¥åŒ…æ‹¬ä¸SQuADç›¸å…³çš„å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ï¼ˆ32/48ï¼‰ã€è®­ç»ƒè½®æ¬¡ç­‰ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä¼šåœ¨PDFä¸­æœç´¢ä¸â€œSQuADâ€ç›¸å…³çš„å†…å®¹ã€‚

æ­£åœ¨è¯»å–æ–‡æ¡£

æˆ‘ä»¬è¿˜ç¼ºå°‘å…³äºæ•°æ®é›†çš„å…·ä½“ç»†èŠ‚ï¼Œå°¤å…¶æ˜¯GLUEä»»åŠ¡åˆ—è¡¨ï¼ˆå·²ç»æ‰¾åˆ°ï¼‰ï¼Œä½†SQuADçš„è®­ç»ƒè®¾ç½®è¿˜æ²¡æœ‰çœ‹åˆ°ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä¼šç»§ç»­åœ¨PDFä¸­æœç´¢ä¸SQuAD v1.1å’Œ2.0ç›¸å…³çš„ä¿¡æ¯ï¼Œçœ‹çœ‹èƒ½æ‰¾åˆ°å“ªäº›å…·ä½“ç»†èŠ‚ã€‚

æ­£åœ¨è¯»å–æ–‡æ¡£

æˆ‘ä»¬å·²ç»æ”¶é›†äº†è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œç°åœ¨å¯ä»¥å¼€å§‹æ’°å†™è¯¦ç»†çš„ä¸­æ–‡åˆ†æï¼ŒåŒ…æ‹¬ï¼š

*   è®ºæ–‡å’Œé™„å½•ç»“æ„è¯´æ˜ï¼Œå¸®åŠ©å®šä½ç»†èŠ‚ã€‚
*   é¢„è®­ç»ƒä»»åŠ¡ä¸è¾“å…¥è¡¨ç¤ºï¼šä¸‰å…ƒç»„åµŒå…¥ï¼›é®ç›–ä»»åŠ¡80/10/10çš„ä¾‹å­ï¼›NSPä»»åŠ¡ç”Ÿæˆï¼ŒNSPå‡†ç¡®ç‡97%-98%ï¼ˆåœ¨03-BERT.pdf L3æ‰¾åˆ°çš„ç»†èŠ‚ï¼‰ã€‚
*   é¢„è®­ç»ƒæ•°æ®é›†ï¼šBooksCorpus 800M + Wikipedia 2,500Mï¼›æ–‡æ¡£çº§ï¼›ç†ç”±ï¼Œå¯å‚è€ƒ\[14\] L36-L46ã€‚

ï¼š

*   é¢„è®­ç»ƒè¶…å‚ï¼šæ‰¹é‡å¤§å°256ï¼Œåºåˆ—é•¿åº¦512ï¼Œè®­ç»ƒæ­¥éª¤1,000,000ï¼ŒAdamå­¦ä¹ ç‡1e-4ï¼Œbetasç­‰ï¼›warmup 10kæ­¥ï¼›L2æ­£åˆ™åŒ–0.01ï¼›dropout 0.1ï¼›geluæ¿€æ´»å‡½æ•°ï¼›è®­ç»ƒè®¡åˆ’ï¼šå‰90%ä½¿ç”¨128é•¿åº¦ï¼Œå10%ä½¿ç”¨512é•¿åº¦ï¼›ç¡¬ä»¶ä½¿ç”¨Cloud TPUï¼Œè®­ç»ƒæ—¶é—´4å¤©ï¼›å¼•ç”¨\[9\] L24-L52å’Œ\[3\]/\[6\]ã€‚
*   å¾®è°ƒé€šç”¨èŒƒå›´ï¼šæ‰¹é‡å¤§å°16/32ï¼Œå­¦ä¹ ç‡{5e-5,3e-5,2e-5}ï¼Œepochs{2,3,4}ï¼›å¾®è°ƒé€Ÿåº¦å¿«ï¼Œ Cloud TPUä¸Š<=1å°æ—¶å³å¯å®Œæˆï¼›å¼•ç”¨\[9\] L54-L83ï¼Œ\[8\] L17-L24ã€‚
*   ä»»åŠ¡ç‰¹å®šå¾®è°ƒï¼š
    *   GLUEï¼šå¾®è°ƒ3è½®ï¼Œæ‰¹é‡å¤§å°32ï¼Œå­¦ä¹ ç‡é€‰æ‹©{5e-5,4e-5,3e-5,2e-5}ï¼Œå¯¹å¤§æ¨¡å‹éšæœºé‡å¯ï¼›å¼•ç”¨\[2\] L13-L23ã€‚
    *   SQuAD v1.1ï¼šå¼€å§‹/ç»“æŸå¾—åˆ†ï¼›è®­ç»ƒ3è½®ï¼Œå­¦ä¹ ç‡5e-5ï¼Œæ‰¹é‡å¤§å°32ï¼›TriviaQAé¢„å¾®è°ƒï¼›å¼•ç”¨\[5\]/\[1\] L41å¼€å§‹ï¼Œè®­ç»ƒ3è½®ã€‚

ï¼š

*   SQuAD v2.0ï¼šä½¿ç”¨ç©ºé˜ˆå€¼å’Œsnullï¼Œè®­ç»ƒ2è½®ï¼Œå­¦ä¹ ç‡5e-5ï¼Œæ‰¹é‡å¤§å°48ï¼›å¼•ç”¨\[0\] L14-L21 å’Œ L44-L49ã€‚
*   SWAGï¼š3è½®è®­ç»ƒï¼Œå­¦ä¹ ç‡2e-5ï¼Œæ‰¹é‡å¤§å°16ï¼›å¼•ç”¨\[0\] L76-L79 å’Œ \[7\] L26-L30ã€‚
*   æ¨¡å‹è§„æ¨¡ï¼šBERTBASE (L=12, H=768, A=12) ä¸ BERTLARGE (L=24, H=1024, A=16)ï¼›å¼•ç”¨\[7\] L35-L38ï¼ˆè¡¨6ï¼‰ã€‚

ï¼š

*   BERTLARGEåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—æ›´å¥½çš„è¡¨ç°ï¼Œå¢åŠ äº†å±‚æ•°ã€éšè—å•å…ƒå’Œå¤´æ•°ï¼Œå…·ä½“ç»†èŠ‚è§\[5\]ã€‚
*   é¢„è®­ç»ƒä»»åŠ¡å®éªŒï¼šå»æ‰NSPã€LTRä¸BiLSTMå¯¹å„ä»»åŠ¡å½±å“ï¼ŒNSPå¯¹QNLIã€MNLIå’ŒSQuADæœ‰è´Ÿé¢å½±å“ï¼›BiLSTMå¯¹SQuADæœ‰æ‰€å¸®åŠ©ï¼Œä½†æ¯”MLMæ•ˆæœå·®ã€‚
*   è®­ç»ƒæ­¥æ•°ï¼šBERTBASEåœ¨1Mæ­¥æ—¶æ¯”500kæ­¥å¤šäº†1%çš„MNLIå‡†ç¡®ç‡ï¼›MLMæ”¶æ•›è¾ƒæ…¢ã€‚

å®Œæˆ

## Response:
2025-09-17 10:23:45

ä¸‹é¢ç»™ä½ ä¸€ä»½â€œåªçœ‹è¿™ä¸€ç¯‡å°±èƒ½å¤ç°å®éªŒâ€çš„è¶…è¯¦ç»†å¤ç›˜ï¼Œè¦†ç›– BERT è®ºæ–‡ä¸é™„å½•ï¼ˆPDF/MDï¼‰é‡Œçš„ï¼šæ•°æ®é›†ã€æ¨¡å‹ä¸è¾“å…¥è®¾è®¡ã€é¢„è®­ç»ƒä¸å¾®è°ƒè¶…å‚ã€ä»»åŠ¡åŒ–ç»†èŠ‚ï¼Œä»¥åŠæ‰€æœ‰èƒ½æ‰¾åˆ°çš„æ¶ˆèå®éªŒè¦ç‚¹ä¸é‡åŒ–ç»“è®ºã€‚æ¯æ¡éƒ½å¯¹åº”åŸæ–‡å¯è¿½æº¯é¡µè¡Œå·ï¼Œä¾¿äºä½ æ ¸å¯¹æˆ–å†™åˆ°å¤ç°å®éªŒè¯´æ˜é‡Œã€‚

* * *

ä¸€ã€å®éªŒæ¡†æ¶ä¸è¾“å…¥è®¾è®¡
===========

**è¾“å…¥ä¸‰é‡åµŒå…¥**ï¼ˆToken + Segment + Positionï¼‰ã€‚BERT çš„è¾“å…¥å‘é‡æ˜¯è¿™ä¸‰è€…é€å…ƒç´ ç›¸åŠ ï¼›è®ºæ–‡ Figure 2 æ¸…æ™°ç¤ºæ„ï¼ˆç»™äº†å…·ä½“ token/segment/position å¯¹é½ï¼‰ã€‚è¿™å°±æ˜¯åé¢æ‰€æœ‰ä»»åŠ¡â€œæ¢å¤´ä¸æ¢èº«â€çš„æ ¹åŸºã€‚03-BERT

**ä¸¤å¤§é¢„è®­ç»ƒä»»åŠ¡**

*   **MLM æ©ç ç­–ç•¥ï¼ˆ15% tokenï¼‰**ï¼šè¢«é€‰ä¸­çš„ token æŒ‰ 80%â†’æ›¿æ¢ä¸º `[MASK]`ï¼Œ10%â†’æ¢æˆéšæœºè¯ï¼Œ10%â†’ä¿æŒä¸å˜ã€‚é™„å½•ç”¨ â€œmy dog is hairyâ€ ä¸¾ä¾‹é€æ¡è¯´æ˜ã€‚è¿™æ ·æ—¢è®©ç¼–ç å™¨ä¸çŸ¥é“æœªæ¥è¢«é—®å“ªä¸ªè¯ã€ä¹Ÿå‡å°‘ä¸‹æ¸¸æ—  `[MASK]` çš„åˆ†å¸ƒåç§»ã€‚è¿˜æŒ‡å‡ºçœŸæ­£è¢«éšæœºæ›¿æ¢åªå å…¨éƒ¨ token çš„ 1.5%ï¼ˆ15%Ã—10%ï¼‰ï¼Œå¯¹è¯­è¨€ç†è§£å‡ ä¹æ— ä¼¤ã€‚03-BERT 03-BERT
*   **NSP å¥å¯¹é¢„æµ‹**ï¼š50% æ­£ä¾‹ â€œæ˜¯ä¸‹ä¸€å¥â€ï¼Œ50% è´Ÿä¾‹ â€œéšæœºå¥â€ã€‚é™„å½•ç»™äº†æ­£/è´Ÿä¾‹æ¨¡æ¿ï¼›è®ºæ–‡è¿˜æåˆ°æœ€ç»ˆæ¨¡å‹åœ¨ NSP ä¸Šèƒ½åˆ° **97%â€“98%** çš„å‡†ç¡®ç‡ï¼ˆè¿™ç‚¹å¸¸è¢«å¿½è§†ï¼Œä½†å®ƒè§£é‡Šäº† \[CLS\] å¥å‘é‡åœ¨æœªå¾®è°ƒå‰å¹¶æ— ç‹¬ç«‹è¯­ä¹‰ï¼‰ã€‚03-BERT 03-BERT

* * *

äºŒã€é¢„è®­ç»ƒï¼šè¯­æ–™ã€é‡‡æ ·ä¸è¶…å‚
==============

**è¯­æ–™ä¸é‡‡æ ·**

*   **BooksCorpusï¼ˆ8 äº¿è¯ï¼‰+ è‹±æ–‡ Wikipediaï¼ˆ25 äº¿è¯ï¼‰**ï¼ŒåšæŒ **document-level** è¯­æ–™è€Œéæ‰“ä¹±çš„å¥å­çº§ï¼ˆå¦‚ Billion Wordï¼‰ï¼Œç†ç”±æ˜¯è¦ä¿è¯é•¿ç‰‡æ®µçš„è¿ç»­æ€§ä»¥æ”¯æŒå¥å¯¹ä¸é•¿åºåˆ—çš„è®­ç»ƒã€‚03-BERT
*   æ¯ä¸ªè®­ç»ƒæ ·æœ¬ä»è¯­æ–™æŠ½ä¸¤æ®µâ€œå¥å­â€ï¼ˆé•¿åº¦å¯å¤§å¯å°ï¼Œåˆå¹¶å â‰¤512ï¼‰ï¼ŒA/B åˆ†åˆ«æ‰“ä¸Š sentence segment A/Bï¼›NSP çš„ 50% è´Ÿé‡‡æ ·åœ¨æ­¤æ­¥å®Œæˆã€‚**MLM åœ¨ WordPiece ä¹‹å** ä»¥ 15% å‡åŒ€æ¦‚ç‡å®æ–½ï¼Œä¸å¯¹å­è¯åšç‰¹æ®Šç…§é¡¾ã€‚03-BERT

**æ ¸å¿ƒè¶…å‚ä¸è°ƒåº¦**ï¼ˆé™„å½• A.2 æ±‡æ€»ï¼‰

*   **æ‰¹å¤§å°**ï¼š256 åºåˆ—ï¼ˆ512 tokenï¼‰â†’æ¯æ‰¹ 128,000 tokenã€‚**æ€»æ­¥æ•°**ï¼š1,000,000ï¼ˆçº¦ 40 epoch è¦†ç›– 33 äº¿è¯ï¼‰ã€‚ä¼˜åŒ–å™¨ **Adam**ï¼ˆlr=1e-4, Î²1=0.9, Î²2=0.999ï¼‰ï¼Œ**L2 æƒé‡è¡°å‡ 0.01**ï¼Œ**å‰ 10k æ­¥çº¿æ€§ warmup** åçº¿æ€§è¡°å‡ï¼›**dropout=0.1**ï¼›**GELU** æ¿€æ´»ï¼›æŸå¤±=MLM + NSPã€‚03-BERT
*   **åºåˆ—é•¿åº¦åˆ†æ®µè®­ç»ƒ**ï¼šå› æ³¨æ„åŠ›å¯¹åºåˆ—é•¿åº¦æ˜¯äºŒæ¬¡å¤æ‚åº¦ï¼Œ**90% æ­¥æ•°ç”¨ L=128**ï¼Œ**æœ€å 10% ç”¨ L=512** å®Œæˆä½ç½®ç¼–ç å¯¹é•¿åºåˆ—çš„é€‚é…ã€‚03-BERT
*   **ç®—åŠ›ä¸è€—æ—¶**ï¼šBERT-Base ç”¨ 4Ã—Cloud TPU Podsï¼ˆ16 èŠ¯ç‰‡ï¼‰ï¼ŒBERT-Large ç”¨ 16Ã—Cloud TPUï¼ˆ64 èŠ¯ç‰‡ï¼‰ï¼Œ**å„è‡ªé¢„è®­ç»ƒçº¦ 4 å¤©**ã€‚03-BERT

> ä¸ OpenAI GPT çš„è®­ç»ƒå·®å¼‚ï¼ˆé™„å½• A.4ï¼‰ï¼šBERT è¯­æ–™æ›´å¤§ï¼ˆBooksCorpus+Wiki vs ä»… BooksCorpusï¼‰ã€BERT é¢„è®­ç»ƒé˜¶æ®µå°±å¼•å…¥ `[CLS]/[SEP]` ä¸å¥å­ A/B åµŒå…¥ã€BERT æ¯æ­¥ 128k è¯Ã—100 ä¸‡æ­¥ï¼Œè€Œ GPT æ¯æ­¥ 32k è¯Ã—100 ä¸‡æ­¥ï¼›BERT ä¸‹æ¸¸æŒ‰å¼€å‘é›†é€‰å­¦ä¹ ç‡è€Œéå›ºå®š 5e-5ã€‚03-BERT

* * *

ä¸‰ã€å¾®è°ƒï¼šé€šç”¨ç½‘æ ¼ä¸ä»»åŠ¡ç»†èŠ‚
==============

**é€šç”¨å¾®è°ƒç½‘æ ¼ï¼ˆé™„å½• A.3ï¼‰**

*   è¶…å‚èŒƒå›´ï¼š**Batch={16, 32}ï¼›LRï¼ˆAdamï¼‰={5e-5, 3e-5, 2e-5}ï¼›Epoch={2,3,4}**ã€‚Dropout ä¸€ç›´ 0.1 ä¸é¢„è®­ç»ƒä¸€è‡´ã€‚å¤§æ•°æ®é›†å¯¹è¶…å‚ä¸æ•æ„Ÿï¼›**å¾®è°ƒå¾ˆå¿«**ï¼Œå¯ç›´æ¥ç½‘æ ¼æœç´¢ dev æœ€ä¼˜ã€‚03-BERT
*   è®ºæ–‡æ˜ç¡®å¼ºè°ƒï¼š**ä»ç›¸åŒé¢„è®­ç»ƒæƒé‡å‡ºå‘ï¼Œæ‰€æœ‰ç»“æœå•å¡ TPU â‰¤1 å°æ—¶å¯å¤ç°**ï¼ˆGPU å‡ å°æ—¶ï¼‰ï¼Œå¹¶ç»™äº† SQuAD Dev F1â‰ˆ91% 30 åˆ†é’Ÿçš„ä¾‹å­ã€‚03-BERT

**GLUEï¼ˆ11 é¡¹ NLUï¼‰**

*   å®æ“ç­–ç•¥ï¼š**æ‰€æœ‰ GLUE ä»»åŠ¡ç»Ÿä¸€ç”¨ batch=32ã€epoch=3**ï¼›LR åœ¨ {5e-5, 4e-5, 3e-5, 2e-5} ä¸Šé€‰ dev æœ€ä¼˜ã€‚BERT-Large åœ¨å°æ•°æ®é›†ä¸Šå¾®è°ƒä¸ç¨³ï¼Œ**éœ€å¤šéšæœºé‡å¯**ï¼ˆç›¸åŒé¢„è®­ç»ƒ checkpointï¼Œä¸åŒæ•°æ®æ‰“ä¹±ä¸åˆ†ç±»å¤´åˆå§‹åŒ–ï¼‰ï¼Œå– dev æœ€ä¼˜ã€‚03-BERT
*   ä»»åŠ¡é‡çº§å‚è€ƒï¼šMNLI 392kã€QQP 363kã€QNLI 108kã€SST-2 67kã€CoLA 8.5kã€STS-B 5.7kã€MRPC 3.5kã€RTE 2.5kï¼ˆè¡¨ 1 ä¸‹æ³¨é‡ŠåŒºåŸŸä¸èŠ‚ 4.1ï¼‰ã€‚03-BERT

**SQuAD v1.1ï¼ˆæŠ½å–å¼ QAï¼‰**

*   ç»“æ„ï¼šä»…æ–°å¢**èµ·ç‚¹å‘é‡ Sã€ç»ˆç‚¹å‘é‡ E**ï¼Œé€šè¿‡ä¸ token è¡¨å¾çš„ç‚¹ç§¯+softmax å–æœ€å¤§åˆæ³•è·¨åº¦ï¼›è®­ç»ƒç›®æ ‡ä¸ºæ­£ç¡®èµ·æ­¢ä½ç½®çš„å¯¹æ•°ä¼¼ç„¶å’Œã€‚**å¾®è°ƒ 3 epochï¼ŒLR=5e-5ï¼Œbatch=32**ï¼›æ›¾ä½¿ç”¨ TriviaQA å…ˆè¡Œå¾®è°ƒå†åœ¨ SQuAD ä¸Šå¾®è°ƒï¼ˆæ•°æ®å¢å¼ºï¼‰ã€‚03-BERT 03-BERT

**SQuAD v2.0ï¼ˆå¯æ— ç­”æ¡ˆï¼‰**

*   æŠ€å·§ï¼šæŠŠâ€œæ— ç­”æ¡ˆâ€è§†ä½œèµ·æ­¢éƒ½åœ¨ `[CLS]` çš„è·¨åº¦ï¼Œå¼•å…¥ **s\_null = SÂ·C + EÂ·C** ä¸æœ€ä¼˜éç©ºè·¨åº¦åˆ†æ•°æ¯”è¾ƒï¼Œ**é˜ˆå€¼ Ï„ åœ¨ dev ä¸Šè°ƒä¼˜**ã€‚**å¾®è°ƒ 2 epochï¼ŒLR=5e-5ï¼Œbatch=48**ã€‚03-BERT 03-BERT

**SWAGï¼ˆå››é€‰ä¸€å¸¸è¯†æ¨ç†ï¼‰**

*   åšæ³•ï¼šå°†â€œå‰æ+æ¯ä¸ªå€™é€‰â€æ‹¼æ¥æˆå››ä¸ªè¾“å…¥ï¼Œ**ä»…åŠ ä¸€ä¸ªæ‰“åˆ†å‘é‡ä¸ \[CLS\] è¡¨å¾ç‚¹ç§¯å softmax**ï¼›**å¾®è°ƒ 3 epochï¼ŒLR=2e-5ï¼Œbatch=16**ã€‚03-BERT 03-BERT

* * *

å››ã€æ¶ˆèå®éªŒï¼ˆAblationsï¼‰
=================

1ï¼‰**é¢„è®­ç»ƒä»»åŠ¡çš„ä½œç”¨ï¼ˆè¡¨ 5ï¼‰**
-------------------

åŒæ ·çš„ BERT-Base æ¶æ„ï¼Œä¸åŒé¢„è®­ç»ƒç›®æ ‡çš„ **Dev** å¯¹æ¯”ï¼ˆMNLI/QNLI/MRPC/SST-2/SQuAD-F1ï¼‰ï¼š

*   **BERT-Baseï¼ˆMLM+NSPï¼‰**ï¼š84.4 / 88.4 / 86.7 / 92.7 / **88.5**ã€‚
*   **å»æ‰ NSPï¼ˆä»… MLMï¼‰**ï¼š83.9 / 84.9 / 86.5 / 92.6 / **87.9** â†’ QNLI/MNLI/SQuAD æ˜æ˜¾ä¸‹æ»‘ï¼ŒNSP å¯¹å¥é—´å…³ç³»ä¸ QA æœ‰å¸®åŠ©ã€‚
*   **æ”¹ä¸º LTR è¯­è¨€æ¨¡å‹ä¸”æ—  NSPï¼ˆGPT é£æ ¼ï¼‰**ï¼š82.1 / 84.3 / 77.5 / 92.1 / **77.8** â†’ **MRPCã€SQuAD å¤§å¹…ä¸‹é™**ï¼Œå·¦åˆ°å³è¡¨å¾åœ¨ token çº§é¢„æµ‹ç¼ºä¹å³ä¾§ä¸Šä¸‹æ–‡ã€‚
*   **åœ¨ LTR æ¨¡å‹ä¸ŠåŠ éšæœºåˆå§‹åŒ– BiLSTM**ï¼šSQuAD ä» 77.8 æ¶¨åˆ° **84.9**ï¼Œä½† GLUE å¤šæ•°æŒ‡æ ‡æ›´å·®ï¼Œä»æ˜¾è‘—ä¸å¦‚åŸç”ŸåŒå‘ Transformerã€‚03-BERT 03-BERT

> ç»“è®ºï¼š**åŒå‘æ€§ + NSP** æ˜¯ BERT æ€§èƒ½é¢†å…ˆçš„å…³é”®ï¼›å•å‘ LM å³ä¾¿å  BiLSTM ä¹Ÿéš¾ä»¥è¡¥é½ã€‚03-BERT

2ï¼‰**è®­ç»ƒæ­¥æ•°ï¼ˆé™„å½• C.1ï¼‰**
------------------

*   é—®é¢˜ä¸€ï¼šBERT æ˜¯å¦çœŸçš„éœ€è¦ **128k è¯/æ­¥ Ã— 100 ä¸‡æ­¥**ï¼Ÿâ†’ **éœ€è¦**ã€‚BERT-Base **1M æ­¥æ¯” 500k æ­¥åœ¨ MNLI è¿˜èƒ½å¤šçº¦ 1.0%**ã€‚
*   é—®é¢˜äºŒï¼šMLM ä¼šä¸ä¼šå› ä¸ºæ¯æ‰¹åªé¢„æµ‹ 15% token è€Œæ”¶æ•›æ›´æ…¢ï¼Ÿâ†’ **æ˜¯çš„ç•¥æ…¢**ï¼Œä½†**å‡ ä¹ç«‹åˆ»åœ¨ç»å¯¹ç²¾åº¦ä¸Šè¶…è¿‡ LTR**ã€‚03-BERT

3ï¼‰**æ©ç ç­–ç•¥ï¼ˆé™„å½• C.2ï¼‰**
------------------

*   ç›®çš„ï¼š**å‡å°‘é¢„è®­ç»ƒ/å¾®è°ƒåˆ†å¸ƒä¸åŒ¹é…**ï¼ˆä¸‹æ¸¸ä»ä¸è§ `[MASK]`ï¼‰ï¼Œå› æ­¤é‡‡ç”¨ 80/10/10 çš„æ··åˆç­–ç•¥ï¼Œå¹¶å¯¹ MNLI ä¸ NERï¼ˆå« feature-based ä¸ fine-tune ä¸¤ç§ï¼‰åœ¨ Dev é›†åšäº†å¯¹æ¯”ã€‚03-BERT capabilities

4ï¼‰**æ¨¡å‹è§„æ¨¡ï¼ˆè¡¨ 6ï¼‰**
---------------

*   è§„æ¨¡ä»ï¼ˆ**#L,#H,#A**ï¼‰= (12,768,12)â†’(24,1024,16) é€æ­¥å¢å¤§ï¼Œ**LM éªŒè¯å›°æƒ‘åº¦**ä¸å¤šé¡¹ GLUE Dev æŒ‡æ ‡**æŒç»­æå‡**ï¼ˆç¤ºä¾‹ï¼š12/768/12 çš„ MLM ppl=3.99ï¼›24/1024/16 ppl=3.23ï¼ŒDev ä¸Š MNLI-m ä» 84.4â†’86.6ï¼‰ã€‚**ç»“è®ºï¼šæ›´å¤§æ¨¡å‹æ˜¾è‘—æ›´å¼º**ï¼Œå°¤å…¶å°æ ·æœ¬ä»»åŠ¡ã€‚03-BERT 03-BERT 03-BERT

5ï¼‰**Fine-tuning vs Feature-basedï¼ˆè¡¨ 7ï¼ŒCoNLL-2003 NERï¼‰**
------------------------------------------------------

*   **å¾®è°ƒ**ï¼šBERT-Large/-Base Dev F1=96.6/96.4ï¼ŒTest F1=92.8/92.4
*   **ç‰¹å¾å¼ï¼ˆç”¨ BERT-Base æå–å±‚è¡¨ç¤º + 2 å±‚ 768 BiLSTMï¼‰**ï¼šè¿æ¥**æœ€åå››å±‚**è¡¨ç°æœ€ä½³ Dev F1=**96.1**ï¼Œä»…æ¯”å¾®è°ƒå·® 0.3ï¼›å…¶ä»–æ‹¼æ³•ä¹Ÿæ¥è¿‘ã€‚â†’ **BERT æ—¢é€‚åˆç«¯åˆ°ç«¯å¾®è°ƒï¼Œä¹Ÿèƒ½å½“å¼ºå¤§ç‰¹å¾å™¨**ã€‚03-BERT 03-BERT

* * *

äº”ã€å„ä»»åŠ¡ä¸»ç»“æœä¸å®æ“æç¤º
=============

**GLUE ä¸»ç»“æœ**ï¼šBERT-Base ä¸ BERT-Large åœ¨**æ‰€æœ‰ä»»åŠ¡**ä¸Šéƒ½æ˜¾è‘—è¶…è¶Šæ—¢æœ‰ç³»ç»Ÿï¼Œå¹³å‡æå‡çº¦ **+4.5% / +7.0%**ï¼›MNLI ä¸Šç»å¯¹æå‡ 4.6%ã€‚**BERT-Large å¯¹å°æ•°æ®é›†ä¼˜åŠ¿æ›´æ˜æ˜¾**ã€‚03-BERT

**SQuAD 1.1 / 2.0**ï¼š

*   v1.1ï¼šå•æ¨¡ 90.9 F1ã€é›†æˆ 91.8 F1ï¼›å…ˆ TriviaQA å† SQuAD çš„ç­–ç•¥å¸¦æ¥é¢å¤–å¢ç›Šã€‚03-BERT 03-BERT
*   v2.0ï¼šç”¨ `[CLS]` ä½œä¸ºæ— ç­”æ¡ˆè·¨åº¦å¹¶è°ƒé˜ˆå€¼ Ï„ï¼ŒBERT-Large Test F1=83.1ï¼›è®ºæ–‡ä¸­ä¸å½“æ—¶ SOTA æ¯”è¾ƒæœ‰ **+5.1 F1** çš„ä¼˜åŠ¿ã€‚03-BERT 03-BERT

**SWAG**ï¼šBERT-Large Dev 86.6 / Test 86.3ï¼Œæ˜¾è‘—é«˜äº GPT ä¸ ESIM+ELMoï¼›å¾®è°ƒä»…éœ€ä¸€ä¸ªå‘é‡æ‰“åˆ†å››ä¸ªå€™é€‰ã€‚03-BERT 03-BERT

* * *

å…­ã€å¯å¤ç°â€œå‚æ•°é€ŸæŸ¥è¡¨â€ï¼ˆè½åœ°å»ºè®®ï¼‰
==================

**é¢„è®­ç»ƒï¼ˆå¦‚éœ€è‡ªè®­ï¼‰**

*   è¯­æ–™ï¼šBooksCorpus + Wikipediaï¼ˆçº¯æ–‡æœ¬ï¼Œåˆ è¡¨æ ¼/åˆ—è¡¨/è¡¨å¤´ï¼‰
*   é‡‡æ ·ï¼šå¥å¯¹ â‰¤512ï¼›MLM=15%ï¼ˆ80/10/10ï¼‰ï¼›NSP=50/50 æ­£è´Ÿ
*   è¶…å‚ï¼šbatch=256Ã—512ï¼Œsteps=1,000,000ï¼›Adam(lr=1e-4, Î²1=0.9, Î²2=0.999, wd=0.01)ï¼Œwarmup=10kï¼Œçº¿æ€§è¡°å‡ï¼›dropout=0.1ï¼›GELUï¼›**L=128 for 90% steps â†’ L=512 for 10%**ï¼›TPU Pod é¢„ä¼° 4 å¤©ã€‚03-BERT

**å¾®è°ƒï¼ˆé»˜è®¤ç½‘æ ¼ï¼‰**

*   Gridï¼šbatchâˆˆ{16,32}ï¼Œlrâˆˆ{5e-5,3e-5,2e-5}ï¼Œepochâˆˆ{2,3,4}ï¼Œå– dev æœ€ä¼˜ï¼›GLUE å¯ç›´æ¥ **batch=32, epoch=3** å¹¶åœ¨ {5e-5,4e-5,3e-5,2e-5} é€‰ lrï¼›BERT-Large å°æ•°æ®å»ºè®® **å¤šæ¬¡éšæœºé‡å¯**ã€‚03-BERT 03-BERT
*   ä»»åŠ¡ç‰¹åŒ–ï¼š
    *   **SQuAD v1.1**ï¼š3 epoch / 5e-5 / batch=32ï¼›**S/E å‘é‡**+è·¨åº¦æ‰“åˆ†ã€‚03-BERT
    *   **SQuAD v2.0**ï¼š2 epoch / 5e-5 / batch=48ï¼›**s\_null = SÂ·C + EÂ·C** ä¸æœ€ä¼˜éç©ºè·¨åº¦æ¯”è¾ƒ + é˜ˆå€¼ Ï„ã€‚03-BERT 03-BERT
    *   **SWAG**ï¼š3 epoch / 2e-5 / batch=16ï¼›**\[CLS\] æ‰“åˆ†å‘é‡**ã€‚03-BERT 03-BERT

**æ¨¡å‹é€‰æ‹©**

*   è‹¥èµ„æºæœ‰é™ï¼šä¼˜å…ˆ **BERT-Base (L=12,H=768,A=12)**ï¼›è‹¥è¿½æ±‚ä¸Šé™ï¼š**BERT-Large (L=24,H=1024,A=16)**ï¼Œå¤šæ•°ä»»åŠ¡éƒ½æœ‰ç¨³å®šæ­£å¢ç›Šï¼Œå°¤å…¶å°æ•°æ®ã€‚03-BERT 03-BERT

* * *

ä¸ƒã€ä½ åœ¨å¤ç°æˆ–æ‰©å±•æ—¶çš„å…³é”®å†³ç­–ç‚¹
================

1.  **æ˜¯å¦ä¿ç•™ NSPï¼Ÿ**  
    å¯¹éœ€è¦å¥é—´æ¨ç†/è·¨å¥ä¸Šä¸‹æ–‡ï¼ˆMNLI/QNLI/QAï¼‰çš„ä»»åŠ¡ï¼Œ**ä¿ç•™ NSP æ›´ç¨³**ï¼›å®Œå…¨å»æ‰ä¼šå¯¼è‡´æ˜¾è‘—ä¸‹é™ã€‚03-BERT
2.  **å•å‘ LM vs åŒå‘ MLMï¼Ÿ**  
    å•å‘ LM åœ¨ token-level ä»»åŠ¡ï¼ˆQA/æ ‡æ³¨ï¼‰å› æ— å³ä¸Šä¸‹æ–‡æ˜æ˜¾åƒäºï¼›å³ä¾¿åŠ  BiLSTM ä¹Ÿè¿½ä¸å›åŒå‘ç¼–ç å™¨ã€‚03-BERT
3.  **è®­ç»ƒæ­¥æ•°å¤Ÿä¸å¤Ÿï¼Ÿ**  
    **100 ä¸‡æ­¥**ç¡®æœ‰å®è¯æ”¶ç›Šï¼Œå°¤å…¶åœ¨ MNLI ä¸Š +1% é‡çº§ã€‚è‹¥é¢„ç®—ç´§ï¼Œå¯å…ˆ 500k æ­¥åšå¯¹æ¯”ã€‚03-BERT
4.  **æ©ç ç­–ç•¥è¦ä¸è¦æ”¹ï¼Ÿ**  
    80/10/10 çš„åŠ¨æœºæ˜¯ç¼©å°é¢„/å¾®è°ƒåˆ†å¸ƒå·®è·ï¼›é™„å½• C.2 è¯å®å®ƒåœ¨ NER/GLUE ä¸Šæ˜¯æ›´å¥½çš„é»˜è®¤ã€‚capabilities
5.  **å¾®è°ƒç¨³å®šæ€§**  
    BERT-Large åœ¨å°æ•°æ®å¯èƒ½ä¸ç¨³ï¼Œ**å¤šæ¬¡é‡å¯**æ˜¯å®˜æ–¹å»ºè®®å·¥ä½œæµã€‚03-BERT

* * *

å¦‚æœä½ æ¥ä¸‹æ¥è¦æŠŠè¿™äº›ç»†èŠ‚å›ºåŒ–æˆ**å¯å·¥ä¸šåŒ–å¤ç°å®éªŒè„šæœ¬**ï¼Œæˆ‘å¯ä»¥ç›´æ¥ç»™ä½ ï¼š

*   ä¸€ä¸ª `configs/bert_finetune.yaml`ï¼ˆå«é€šç”¨ç½‘æ ¼ + ä»»åŠ¡ç‰¹åŒ–æ®µè½ï¼‰ï¼Œ
*   `train_glue.py / train_squad.py / train_swag.py` ä¸‰å¥—æœ€å°å¯å¤ç°å…¥å£ï¼Œ
*   ä»¥åŠ `README_å¤ç°å®éªŒ.md`ï¼ŒæŠŠä¸Šé¢æ‰€æœ‰â€œé¡µ-è¡Œå·â€é”šç‚¹ä½œä¸ºâ€œå®¡è®¡çº¿ç´¢â€åˆ—è¿›å»ï¼Œæ–¹ä¾¿è®ºæ–‡å®¡ç¨¿å¼æ ¸å¯¹ã€‚éœ€è¦çš„è¯æˆ‘ç°åœ¨å°±æŠŠä»£ç å’Œ YAML å¡«å¥½ã€‚

## Prompt:
2025-09-17 10:23:52

Plan and build bert-ablation pluggable factory-mode project!
first design file category
then give me code file by file

## Response:
2025-09-17 10:25:43

å·²æ€è€ƒ 1m 47s

æˆ‘éœ€è¦è®¾è®¡ä¸€ä¸ªâ€œbert-ablationå¯æ’æ‹”å·¥å‚æ¨¡å¼é¡¹ç›®â€ï¼Œè®¡åˆ’æ–‡ä»¶ç±»åˆ«ï¼Œç„¶åé€æ­¥æä¾›ä»£ç ã€‚ç”¨æˆ·è¦æ±‚å·¥ä¸šçº§ã€æ¨¡å—åŒ–çš„é¡¹ç›®ï¼Œå¯èƒ½ä½¿ç”¨Pythonçš„PyTorchå’ŒHuggingFaceã€‚ä¹‹å‰æåˆ°BERTæ¶ˆèç»†èŠ‚ï¼Œéœ€æ±‚åŒ…æ‹¬åˆ‡æ¢æ¶ˆèï¼ˆæ¯”å¦‚NSPå¼€å…³ã€LTRä¸MLMï¼‰ã€BiLSTMã€æ©ç ç­–ç•¥ç­‰ã€‚ä»£ç è¦æ±‚æ¸…æ™°å¹¶å¸¦æœ‰æ³¨é‡Šã€ç±»å‹æç¤ºã€Loguruã€TensorBoardç­‰å·¥å…·ã€‚

æˆ‘éœ€è¦ç¡®ä¿åªæä¾›å¿…è¦çš„æ ¸å¿ƒæ¨¡å—ï¼šä¸€ä¸ªæ„å»ºæ¨¡å‹å˜ä½“çš„å·¥å‚ï¼Œæ•°æ®é›†ï¼Œè®­ç»ƒå’Œè¯„ä¼°ä»»åŠ¡ï¼Œå¦‚GLUE/SQuADï¼Œè¿™å¯èƒ½ä¼šå¤ªå¤æ‚ã€‚æˆ‘åº”è¯¥ä¸“æ³¨äºè®¾è®¡ä¸€ä¸ªæœ€å°æ•°æ®é›†æ¥å£å’Œè™šæ‹Ÿä»»åŠ¡ï¼ŒåŒæ—¶ä¿è¯å·¥ä¸šçº§å“è´¨ã€‚å»ºè®®å¯åŠ¨ä¸€ä¸ªæ¡†æ¶é¡¹ç›®ï¼ŒåŒ…æ‹¬å¯æ’æ‹”çš„æ¶ˆèå’Œè®­ç»ƒå·¥å…·ã€‚

æˆ‘éœ€è¦æ„å»ºä¸€ä¸ªç¬¦åˆè¦æ±‚çš„é¡¹ç›®ç»“æ„ï¼šä½¿ç”¨pipæ‰“åŒ…çš„æ–¹å¼ï¼Œç»“æ„åŒ…æ‹¬README.mdã€é…ç½®æ–‡ä»¶ã€BERTå·¥å‚ç­‰æ¨¡å—ã€‚é…ç½®å°†åŒ…æ‹¬ä¸åŒçš„æ¶ˆèæ–¹æ¡ˆå’Œä»»åŠ¡ï¼ˆå¦‚GLUEã€SQuADï¼‰ã€‚è¿˜éœ€è¦å®ç°æ—¥å¿—ã€éšæœºç§å­ç®¡ç†ã€å¤šèŠ‚ç‚¹è®­ç»ƒç­‰å®ç”¨å·¥å…·æ¨¡å—ï¼Œç¡®ä¿ä»£ç æ•´æ´ã€‚

ä¸ºäº†å‡å°‘é¡¹ç›®è§„æ¨¡ï¼Œæˆ‘å°†å®ç°å…³é”®æ¨¡å—ï¼štokenization.pyã€collators.pyã€æ•°æ®åŠ è½½ã€æ¨¡å‹å®šä¹‰ã€è®­ç»ƒè¿‡ç¨‹ç­‰ã€‚é‡ç‚¹æä¾›é¢„è®­ç»ƒå’Œå¾®è°ƒçš„CLIæ¥å£ã€æ ·ä¾‹é…ç½®æ–‡ä»¶åŠGLUEå’ŒSQuADç®€åŒ–æ•°æ®é›†æ¥å£ã€‚BiLSTMå’Œå…¶ä»–æ¨¡å—å°†ä½œä¸ºé€‰é¡¹ï¼Œå¹¶å¯æ ¹æ®éœ€è¦æ‰©å±•ã€‚

è€ƒè™‘åˆ°éœ€æ±‚å’Œå¤æ‚æ€§ï¼Œæˆ‘æ‰“ç®—è®¾è®¡ä¸€ä¸ªç²¾ç®€çš„é¡¹ç›®æ¶æ„ã€‚æ ¸å¿ƒéƒ¨åˆ†åŒ…æ‹¬ï¼šæ¨¡å‹è®­ç»ƒä¸å¾®è°ƒçš„é…ç½®æ–‡ä»¶ã€æ•°æ®é¢„å¤„ç†ã€BERTæ¨¡å‹åŠå…¶å˜ç§çš„å®ç°ã€‚ä¸»è¦å®ç°å…³äºNSPã€æ©ç ç­–ç•¥å’ŒBiLSTMçš„åŠŸèƒ½ï¼ŒåŒæ—¶è®°å¾—åŠ å…¥å¿…è¦çš„ä¾èµ–å’ŒTensorBoardæ”¯æŒã€‚LTRéƒ¨åˆ†ä½œä¸ºå¾…å®ç°åŠŸèƒ½æ ‡è®°ã€‚

å®Œæˆ

å¤ªå¥½äº†ï¼ä¸‹é¢æˆ‘ç»™ä½ â€œBERT-Ablation å¯æ’æ‹”å·¥å‚é¡¹ç›®â€çš„**é¡¹ç›®ç»“æ„è®¾è®¡**ï¼Œéšå**é€æ–‡ä»¶ç»™å‡ºå¯ç›´æ¥è¿è¡Œçš„å·¥ä¸šçº§ä»£ç **ï¼ˆPyTorch + ğŸ¤—Transformers + Datasets + Loguru + TensorBoard + YAML é…ç½®ï¼Œæ”¯æŒ MLM+NSPã€MLM onlyã€ï¼ˆå¯é€‰ï¼‰LTRã€BiLSTM é¡¶å±‚ã€æ©ç ç­–ç•¥ 80/10/10 ç­‰å¼€å…³ï¼‰ã€‚ä»£ç éµå¾ª Google é£æ ¼ Docstringã€PEP-8ï¼Œå¸¦ä¸­æ–‡å…³é”®æ³¨é‡Šä¸å¥å£®çš„è¾¹ç•Œæ£€æŸ¥ã€‚

> è¯´æ˜
> 
> 1.  é¢„è®­ç»ƒï¼šé»˜è®¤ç¤ºä¾‹ä½¿ç”¨ `bookcorpusopen` + `wikipedia`ï¼ˆHF å¯è·å–ï¼‰æ¨¡æ‹Ÿè®ºæ–‡è¯­æ–™ç®¡çº¿ï¼›
> 2.  å¾®è°ƒï¼šæä¾› **SST-2**ï¼ˆGLUE åˆ†ç±»ï¼‰ä¸ **SQuAD v1.1**ï¼ˆæŠ½å–å¼ QAï¼‰ä¸¤ä¸ªå®Œæ•´å…¥å£ï¼›
> 3.  æ¶ˆèï¼šé€šè¿‡ YAML æ‰“å¼€/å…³é—­ NSPã€å˜æ›´æ©ç ç­–ç•¥ã€é€‰æ‹© BiLSTM å¤´ã€ï¼ˆå¯é€‰ï¼‰LTRï¼›
> 4.  LTRï¼šåŸºäº `BertLMHeadModel`ï¼ˆè‹¥ç¯å¢ƒä¸å¯ç”¨ä¼šä¼˜é›…é™çº§å¹¶æç¤ºï¼‰ã€‚
>     

* * *

ä¸€ã€é¡¹ç›®æ ‘ï¼ˆfactory-modeï¼Œå¯æ’æ‹”ï¼‰
=======================

```
bert-ablation-factory/
â”œâ”€ requirements.txt
â”œâ”€ README.md
â”œâ”€ configs/
â”‚  â”œâ”€ base.yaml                         # å…¨å±€/è®­ç»ƒé»˜è®¤
â”‚  â”œâ”€ pretrain/
â”‚  â”‚  â”œâ”€ mlm_nsp_base.yaml              # MLM+NSPï¼ˆè®ºæ–‡é»˜è®¤ï¼‰
â”‚  â”‚  â”œâ”€ mlm_no_nsp_base.yaml           # ä»… MLMï¼ˆå» NSP æ¶ˆèï¼‰
â”‚  â”‚  â””â”€ ltr_no_nsp_base.yaml           # LTRï¼ˆå•å‘ï¼‰+ æ—  NSPï¼ˆå¯é€‰ï¼‰
â”‚  â””â”€ finetune/
â”‚     â”œâ”€ glue_sst2_base.yaml            # SST-2 åˆ†ç±»å¾®è°ƒ
â”‚     â””â”€ squad_v1_base.yaml             # SQuAD v1.1 æŠ½å–å¼ QA å¾®è°ƒ
â”œâ”€ bert_ablation_factory/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ registry.py                       # å¯æ’æ‹”æ³¨å†Œè¡¨ï¼ˆmasker/objective/head/taskï¼‰
â”‚  â”œâ”€ utils/
â”‚  â”‚  â”œâ”€ logging.py
â”‚  â”‚  â”œâ”€ io.py
â”‚  â”‚  â”œâ”€ seed.py
â”‚  â”‚  â””â”€ tb.py
â”‚  â”œâ”€ data/
â”‚  â”‚  â”œâ”€ tokenization.py
â”‚  â”‚  â”œâ”€ collators.py                   # MLM 80/10/10ã€NSP å¥å¯¹é‡‡æ ·ã€LTR collator
â”‚  â”‚  â”œâ”€ glue.py                        # SST-2
â”‚  â”‚  â””â”€ squad.py                       # SQuAD v1.1
â”‚  â”œâ”€ modeling/
â”‚  â”‚  â”œâ”€ build.py                       # å·¥å‚ï¼šæŒ‰æ¶ˆèç»„è£…æ¨¡å‹
â”‚  â”‚  â”œâ”€ objectives.py                  # MLM / NSP / LTR æŸå¤±ç»„åˆï¼ˆè–„å°è£…ï¼‰
â”‚  â”‚  â”œâ”€ heads.py                       # åˆ†ç±»/QA å¤´ï¼ˆå«å¯é€‰ BiLSTM å˜ä½“ï¼‰
â”‚  â”‚  â””â”€ bilstm.py                      # è½»é‡ BiLSTM æ¨¡å—
â”‚  â”œâ”€ trainer/
â”‚  â”‚  â”œâ”€ engine.py                      # è®­ç»ƒ/éªŒè¯å¾ªç¯ï¼ˆAMP+TB+Checkpointsï¼‰
â”‚  â”‚  â”œâ”€ optimizer.py                   # AdamW + æƒé‡è¡°å‡
â”‚  â”‚  â”œâ”€ schedulers.py                  # linear warmup-decay
â”‚  â”‚  â””â”€ eval.py                        # åˆ†ç±» accã€SQuAD EM/F1ï¼ˆç®€å®ç°ï¼‰
â”‚  â””â”€ cli/
â”‚     â”œâ”€ pretrain.py                    # é¢„è®­ç»ƒå…¥å£ï¼ˆå¯é€‰æµå¼åŠ è½½ï¼‰
â”‚     â”œâ”€ finetune_classification.py     # SST-2
â”‚     â””â”€ finetune_qa.py                 # SQuAD v1.1
```

* * *

äºŒã€é€æ–‡ä»¶ä»£ç 
=======

1) `requirements.txt`
---------------------

```txt
torch>=2.1
transformers>=4.42
datasets>=2.20
evaluate>=0.4.2
pyyaml>=6.0
loguru>=0.7.2
tqdm>=4.66
tensorboard>=2.16
```

2) `README.md`
--------------

````md
# BERT Ablation Factory (Pluggable)

- Pretrain (MLM+NSP / MLM-only / LTR) and Finetune (SST-2, SQuAD v1.1).
- Toggle ablations via YAML: NSP on/off, 80/10/10 masking, BiLSTM heads, LTR (optional).

## Quickstart

```bash
pip install -r requirements.txt

# Pretrain (MLM+NSP)
python -m bert_ablation_factory.cli.pretrain --cfg configs/pretrain/mlm_nsp_base.yaml

# Finetune SST-2 (GLUE)
python -m bert_ablation_factory.cli.finetune_classification --cfg configs/finetune/glue_sst2_base.yaml

# Finetune SQuAD v1.1
python -m bert_ablation_factory.cli.finetune_qa --cfg configs/finetune/squad_v1_base.yaml
````

TensorBoard: `tensorboard --logdir runs`

````

## 3) `configs/base.yaml`
```yaml
SEED: 42
OUTPUT_DIR: runs
FP16: true
LOG_EVERY: 50
SAVE_EVERY: 1000
EVAL_EVERY: 1000
GRAD_ACCUM_STEPS: 1

MODEL:
  name: bert-base-uncased
  revision: main

DATA:
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true
  max_seq_len: 128

OPTIM:
  lr: 1.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8
  warmup_steps: 10000
  max_steps: 100000

TRAIN:
  per_device_batch_size: 32
  max_steps: 100000
  eval_steps: 1000
  save_steps: 1000
````

4) `configs/pretrain/mlm_nsp_base.yaml`
---------------------------------------

```yaml
# è®ºæ–‡é»˜è®¤ï¼šMLM + NSPï¼›æ©ç  80/10/10ï¼›åºåˆ—é•¿åº¦å¯åç»­æå‡åˆ° 512
_base_: ../base.yaml

ABLATION:
  objective: mlm_nsp         # mlm_only | mlm_nsp | ltr
  mask_strategy: 80_10_10    # 80% [MASK], 10% random, 10% keep
  use_bilstm_head: false

DATASET:
  name: books_wiki_mixed     # ä½¿ç”¨ bookcorpusopen + wikipedia çš„æ··åˆç¤ºä¾‹
  streaming: true
  max_seq_len: 128

TRAIN:
  per_device_batch_size: 32
  max_steps: 100000
  eval_steps: 2000
  save_steps: 2000
```

5) `configs/pretrain/mlm_no_nsp_base.yaml`
------------------------------------------

```yaml
_base_: ../base.yaml
ABLATION:
  objective: mlm_only
  mask_strategy: 80_10_10
  use_bilstm_head: false

DATASET:
  name: books_wiki_mixed
  streaming: true
  max_seq_len: 128
```

6) `configs/pretrain/ltr_no_nsp_base.yaml`
------------------------------------------

```yaml
# å¯é€‰ï¼šå•å‘ LTRï¼ˆè‹¥ BertLMHeadModel ä¸å¯ç”¨å°†ä¼˜é›…é™çº§ä¸ºæŠ¥é”™æç¤ºï¼‰
_base_: ../base.yaml
ABLATION:
  objective: ltr
  mask_strategy: none
  use_bilstm_head: false

DATASET:
  name: books_wiki_mixed
  streaming: true
  max_seq_len: 128
```

7) `configs/finetune/glue_sst2_base.yaml`
-----------------------------------------

```yaml
_base_: ../base.yaml

TASK:
  name: glue_sst2

MODEL:
  name: bert-base-uncased
  revision: main

DATA:
  max_seq_len: 128

TRAIN:
  per_device_batch_size: 32
  num_epochs: 3
  eval_steps: 200
  save_steps: 1000

OPTIM:
  lr: 5.0e-5
  weight_decay: 0.01
  warmup_steps: 500
```

8) `configs/finetune/squad_v1_base.yaml`
----------------------------------------

```yaml
_base_: ../base.yaml

TASK:
  name: squad_v1

MODEL:
  name: bert-base-uncased
  revision: main

DATA:
  max_seq_len: 384
  doc_stride: 128
  max_query_len: 64

TRAIN:
  per_device_batch_size: 16
  num_epochs: 3
  eval_steps: 1000
  save_steps: 1000

OPTIM:
  lr: 5.0e-5
  weight_decay: 0.01
  warmup_steps: 500
```

* * *

9) `bert_ablation_factory/__init__.py`
--------------------------------------

```python
__all__ = ["__version__"]
__version__ = "0.1.0"
```

10) `bert_ablation_factory/registry.py`
---------------------------------------

```python
from __future__ import annotations
from typing import Any, Callable, Dict

class Registry:
    """ç®€å•å¯æ’æ‹”æ³¨å†Œè¡¨ã€‚"""

    def __init__(self, name: str) -> None:
        self._name = name
        self._obj: Dict[str, Any] = {}

    def register(self, key: str) -> Callable[[Any], Any]:
        def deco(fn: Any) -> Any:
            if key in self._obj:
                raise KeyError(f"{self._name} already has key: {key}")
            self._obj[key] = fn
            return fn
        return deco

    def get(self, key: str) -> Any:
        if key not in self._obj:
            raise KeyError(f"{self._name} missing key: {key}")
        return self._obj[key]

    def keys(self) -> list[str]:
        return list(self._obj.keys())

MASKERS = Registry("masker")
OBJECTIVES = Registry("objective")
HEADS = Registry("head")
TASKS = Registry("task")
```

11) `bert_ablation_factory/utils/logging.py`
--------------------------------------------

```python
from __future__ import annotations
from loguru import logger
import sys
from pathlib import Path

def setup_logger(log_file: Path | None = None) -> None:
    """é…ç½® Loguru æ—¥å¿—è¾“å‡ºã€‚"""
    logger.remove()
    logger.add(sys.stderr, level="INFO",
               format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
                      "<level>{level: <8}</level> | "
                      "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - "
                      "<level>{message}</level>")
    if log_file:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        logger.add(str(log_file), level="INFO", rotation="10 MB", retention="10 files")
```

12) `bert_ablation_factory/utils/io.py`
---------------------------------------

```python
from __future__ import annotations
from typing import Any, Dict
from pathlib import Path
import yaml

def load_yaml(path: str | Path) -> Dict[str, Any]:
    """åŠ è½½ YAML ä¸º dictã€‚"""
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def merge_dict(a: dict, b: dict) -> dict:
    """æµ…å±‚åˆå¹¶ï¼šb è¦†ç›– aã€‚"""
    out = dict(a)
    out.update(b)
    return out
```

13) `bert_ablation_factory/utils/seed.py`
-----------------------------------------

```python
from __future__ import annotations
import os
import random
import numpy as np
import torch

def fix_seed(seed: int) -> None:
    """å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°ã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
```

14) `bert_ablation_factory/utils/tb.py`
---------------------------------------

```python
from __future__ import annotations
from pathlib import Path
from torch.utils.tensorboard import SummaryWriter

def create_tb_writer(out_dir: Path, name: str) -> SummaryWriter:
    """åˆ›å»º TensorBoard å†™å…¥å™¨ã€‚"""
    run_dir = out_dir / name
    run_dir.mkdir(parents=True, exist_ok=True)
    return SummaryWriter(log_dir=str(run_dir))
```

15) `bert_ablation_factory/data/tokenization.py`
------------------------------------------------

```python
from __future__ import annotations
from typing import Dict, Any
from transformers import AutoTokenizer

def build_tokenizer(cfg: Dict[str, Any]):
    """æ„å»ºåˆ†è¯å™¨ï¼ˆæ”¯æŒ fastï¼‰ã€‚"""
    tok = AutoTokenizer.from_pretrained(cfg["MODEL"]["name"], revision=cfg["MODEL"].get("revision", "main"), use_fast=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token or tok.sep_token or "[PAD]"
    return tok
```

16) `bert_ablation_factory/data/collators.py`
---------------------------------------------

```python
from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, List, Any, Tuple
import random
import torch

@dataclass
class MLMConfig:
    mask_prob: float = 0.15
    mask_strategy: str = "80_10_10"  # or "100_mask"
    pad_token_id: int = 0
    mask_token_id: int = 103

def _apply_mlm(tokens: torch.Tensor, attention_mask: torch.Tensor, cfg: MLMConfig) -> Tuple[torch.Tensor, torch.Tensor]:
    """å¯¹ batch è¾“å…¥åº”ç”¨ MLM æ©ç ç­–ç•¥ã€‚"""
    labels = tokens.clone()
    # ä¸åœ¨ padding çš„ä½ç½®æ‰èƒ½è¢«é€‰ä¸­
    probability_matrix = (attention_mask == 1).float() * cfg.mask_prob
    masked_indices = torch.bernoulli(probability_matrix).bool()

    labels[~masked_indices] = -100  # ä»…è®¡ç®—è¢«æ©ç  token çš„ loss

    if cfg.mask_strategy == "80_10_10":
        # 80% -> [MASK]
        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices
        tokens[indices_replaced] = cfg.mask_token_id
        # 10% -> random token
        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced
        random_words = torch.randint_like(tokens, low=0, high=tokens.max().item() + 1)
        tokens[indices_random] = random_words[indices_random]
        # å‰©ä½™ 10% ä¿æŒä¸å˜
    elif cfg.mask_strategy == "100_mask":
        tokens[masked_indices] = cfg.mask_token_id
    else:
        raise ValueError(f"Unknown mask_strategy: {cfg.mask_strategy}")
    return tokens, labels

class MLMNSPCollator:
    """MLM+NSP å¥å¯¹ç»„ batch collatorã€‚
    æœŸæœ›è¾“å…¥æ ·æœ¬å·²æ„é€ æˆ (input_ids, token_type_ids, attention_mask, next_sentence_label)ã€‚
    """

    def __init__(self, mlm_cfg: MLMConfig) -> None:
        self.mlm_cfg = mlm_cfg

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        batch = {k: torch.tensor([f[k] for f in features], dtype=torch.long)
                 for k in ["input_ids", "token_type_ids", "attention_mask", "next_sentence_label"]}
        input_ids, labels = _apply_mlm(batch["input_ids"], batch["attention_mask"], self.mlm_cfg)
        batch["input_ids"] = input_ids
        batch["labels"] = labels            # MLM labels
        batch["next_sentence_label"] = batch["next_sentence_label"]
        return batch

class MLMOnlyCollator:
    """ä»… MLM çš„ collatorï¼ˆå•å¥æˆ–æ‹¼æ¥å½¢å¼å‡å¯ï¼‰ã€‚"""

    def __init__(self, mlm_cfg: MLMConfig) -> None:
        self.mlm_cfg = mlm_cfg

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        batch = {k: torch.tensor([f[k] for f in features], dtype=torch.long)
                 for k in ["input_ids", "token_type_ids", "attention_mask"]}
        input_ids, labels = _apply_mlm(batch["input_ids"], batch["attention_mask"], self.mlm_cfg)
        batch["input_ids"] = input_ids
        batch["labels"] = labels
        return batch

class LTRCollator:
    """LTRï¼ˆå•å‘è¯­è¨€æ¨¡å‹ï¼‰collatorã€‚
    ç®€åŒ–ï¼šæ ‡ç­¾ä¸ºä¸‹ä¸€ä¸ª tokenï¼ˆå³ç§»ï¼‰ï¼Œpad ä¸º -100ã€‚æ³¨æ„ï¼šéœ€é…åˆ BertLMHeadModel / is_decoder=True ä½¿ç”¨ã€‚
    """

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        batch = {k: torch.tensor([f[k] for f in features], dtype=torch.long)
                 for k in ["input_ids", "attention_mask"]}
        # æ„é€  causal LM çš„ labelsï¼ˆå³ç§»ï¼‰
        input_ids = batch["input_ids"]
        labels = input_ids.clone()
        labels[:, :-1] = input_ids[:, 1:]
        labels[:, -1] = -100  # æœ€åä¸€ä¸ªæ²¡æœ‰ä¸‹ä¸€ä¸ªè¯
        batch["labels"] = labels
        return batch
```

17) `bert_ablation_factory/data/glue.py`
----------------------------------------

```python
from __future__ import annotations
from typing import Dict, Any, Tuple
from datasets import load_dataset
from transformers import PreTrainedTokenizerBase

def load_sst2(tokenizer: PreTrainedTokenizerBase, max_len: int) -> Tuple[Any, Any]:
    """åŠ è½½ GLUE/SST-2 å¹¶åˆ†è¯ã€‚"""
    ds = load_dataset("glue", "sst2")
    def preprocess(batch):
        enc = tokenizer(batch["sentence"], truncation=True, max_length=max_len, padding="max_length")
        enc["labels"] = batch["label"]
        return enc
    ds = ds.map(preprocess, batched=True, remove_columns=ds["train"].column_names)
    ds.set_format(type="torch")
    return ds["train"], ds["validation"]
```

18) `bert_ablation_factory/data/squad.py`
-----------------------------------------

```python
from __future__ import annotations
from typing import Dict, Any, Tuple
from datasets import load_dataset
from transformers import PreTrainedTokenizerBase

def load_squad_v1(tokenizer: PreTrainedTokenizerBase, max_seq_len: int, doc_stride: int, max_query_len: int):
    """åŠ è½½ SQuAD v1.1 å¹¶è¿›è¡Œæ»‘çª—åˆ‡åˆ†ä¸ç‰¹å¾åŒ–ï¼ˆç®€å®ç°ï¼‰ã€‚"""
    ds = load_dataset("squad")

    def prepare_train_features(examples):
        questions = [q.strip() for q in examples["question"]]
        enc = tokenizer(
            questions,
            examples["context"],
            truncation="only_second",
            max_length=max_seq_len,
            stride=doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length",
        )
        sample_mapping = enc.pop("overflow_to_sample_mapping")
        offset_mapping = enc.pop("offset_mapping")
        start_positions, end_positions = [], []
        for i, offsets in enumerate(offset_mapping):
            input_ids = enc["input_ids"][i]
            cls_index = input_ids.index(tokenizer.cls_token_id)
            sample_idx = sample_mapping[i]
            answer = examples["answers"][sample_idx]
            if len(answer["answer_start"]) == 0:
                start_positions.append(cls_index)
                end_positions.append(cls_index)
                continue
            start_char = answer["answer_start"][0]
            end_char = start_char + len(answer["text"][0])
            sequence_ids = enc.sequence_ids(i)
            # æ‰¾åˆ° context çš„ token èµ·æ­¢
            idx = 0
            while idx < len(sequence_ids) and sequence_ids[idx] != 1:
                idx += 1
            context_start = idx
            while idx < len(sequence_ids) and sequence_ids[idx] == 1:
                idx += 1
            context_end = idx - 1
            # å¦‚æœç­”æ¡ˆä¸åœ¨å½“å‰æ»‘çª—ï¼Œæ ‡ä¸º CLS
            if not (offsets[context_start][0] <= start_char and offsets[context_end][1] >= end_char):
                start_positions.append(cls_index)
                end_positions.append(cls_index)
            else:
                # ç²¾ç¡®å¯¹é½åˆ° token
                start_token = context_start
                while start_token <= context_end and offsets[start_token][0] <= start_char:
                    start_token += 1
                end_token = context_end
                while end_token >= context_start and offsets[end_token][1] >= end_char:
                    end_token -= 1
                start_positions.append(start_token - 1)
                end_positions.append(end_token + 1)
        enc["start_positions"] = start_positions
        enc["end_positions"] = end_positions
        return enc

    train_ds = ds["train"].map(prepare_train_features, batched=True, remove_columns=ds["train"].column_names)
    valid_ds = ds["validation"].map(prepare_train_features, batched=True, remove_columns=ds["validation"].column_names)
    train_ds.set_format(type="torch")
    valid_ds.set_format(type="torch")
    return train_ds, valid_ds
```

19) `bert_ablation_factory/modeling/bilstm.py`
----------------------------------------------

```python
from __future__ import annotations
import torch
from torch import nn

class BiLSTMEncoder(nn.Module):
    """è½»é‡ BiLSTMï¼šå¯ç”¨äºåœ¨ BERT é¡¶éƒ¨å†å ä¸€å±‚ï¼ˆæ¶ˆèç”¨ï¼‰ã€‚"""

    def __init__(self, hidden_size: int, num_layers: int = 1, dropout: float = 0.1) -> None:
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size // 2,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0.0,
            bidirectional=True,
        )

    def forward(self, x: torch.Tensor, mask: torch.Tensor | None = None) -> torch.Tensor:
        lengths = mask.long().sum(-1) if mask is not None else None
        out, _ = self.lstm(x)
        return out
```

20) `bert_ablation_factory/modeling/heads.py`
---------------------------------------------

```python
from __future__ import annotations
import torch
from torch import nn
from .bilstm import BiLSTMEncoder

class ClassificationHead(nn.Module):
    """[CLS] åˆ†ç±»å¤´ï¼Œå¯é€‰ BiLSTM è¿‡æ¸¡ã€‚"""

    def __init__(self, hidden_size: int, num_labels: int, use_bilstm: bool = False) -> None:
        super().__init__()
        self.use_bilstm = use_bilstm
        if use_bilstm:
            self.bridge = BiLSTMEncoder(hidden_size)
        else:
            self.bridge = None
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(hidden_size, num_labels)

    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        # hidden_states: (B, T, H)
        x = hidden_states
        if self.use_bilstm:
            x = self.bridge(x, attention_mask)
        cls = x[:, 0, :]
        cls = self.dropout(cls)
        return self.classifier(cls)

class SpanHead(nn.Module):
    """SQuAD èµ·æ­¢ä½ç½®å¤´ï¼Œå¯é€‰ BiLSTM è¿‡æ¸¡ã€‚"""

    def __init__(self, hidden_size: int, use_bilstm: bool = False) -> None:
        super().__init__()
        self.use_bilstm = use_bilstm
        if use_bilstm:
            self.bridge = BiLSTMEncoder(hidden_size)
        else:
            self.bridge = None
        self.qa_outputs = nn.Linear(hidden_size, 2)

    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        x = hidden_states
        if self.use_bilstm:
            x = self.bridge(x, attention_mask)
        logits = self.qa_outputs(x)  # (B, T, 2)
        start_logits, end_logits = logits.split(1, dim=-1)
        return start_logits.squeeze(-1), end_logits.squeeze(-1)
```

21) `bert_ablation_factory/modeling/objectives.py`
--------------------------------------------------

```python
from __future__ import annotations
from typing import Dict, Any, Tuple
import torch
from torch import nn

class LossCombine(nn.Module):
    """å°†ä¸åŒé¢„è®­ç»ƒæŸå¤±ç»„åˆï¼ˆä¾‹å¦‚ MLM + NSPï¼‰ã€‚"""

    def __init__(self, use_mlm: bool = True, use_nsp: bool = False, use_ltr: bool = False) -> None:
        super().__init__()
        self.use_mlm = use_mlm
        self.use_nsp = use_nsp
        self.use_ltr = use_ltr
        self.ce = nn.CrossEntropyLoss()

    def forward(self, outputs: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, float]]:
        losses = {}
        total = torch.tensor(0.0, device=outputs["hidden_states"].device)
        if self.use_mlm:
            # outputs["mlm_logits"]: (B, T, V); labels: (B, T)
            mlm_loss = self.ce(outputs["mlm_logits"].permute(0, 2, 1), outputs["mlm_labels"])
            losses["mlm"] = mlm_loss
            total = total + mlm_loss
        if self.use_nsp:
            nsp_loss = self.ce(outputs["nsp_logits"], outputs["next_sentence_label"])
            losses["nsp"] = nsp_loss
            total = total + nsp_loss
        if self.use_ltr:
            ltr_loss = self.ce(outputs["ltr_logits"].permute(0, 2, 1), outputs["ltr_labels"])
            losses["ltr"] = ltr_loss
            total = total + ltr_loss
        return total, {k: v.item() for k, v in losses.items()}
```

22) `bert_ablation_factory/modeling/build.py`
---------------------------------------------

```python
from __future__ import annotations
from typing import Dict, Any
from transformers import (BertModel, BertConfig, BertForPreTraining, BertForMaskedLM,
                          BertForSequenceClassification, BertForQuestionAnswering)
import torch
from .heads import ClassificationHead, SpanHead

def build_pretrain_model(cfg: Dict[str, Any], ablation: str):
    """æ ¹æ®æ¶ˆèç›®æ ‡æ„å»ºé¢„è®­ç»ƒæ¨¡å‹ï¼š
    - mlm_nsp: BertForPreTraining
    - mlm_only: BertForMaskedLM
    - ltr: BertLMHeadModelï¼ˆè‹¥ä¸å¯ç”¨åˆ™æŠ¥é”™æç¤ºï¼‰
    """
    name = cfg["MODEL"]["name"]
    if ablation == "mlm_nsp":
        model = BertForPreTraining.from_pretrained(name)
        kind = "mlm_nsp"
    elif ablation == "mlm_only":
        model = BertForMaskedLM.from_pretrained(name)
        kind = "mlm_only"
    elif ablation == "ltr":
        try:
            from transformers import BertLMHeadModel
        except Exception as e:
            raise RuntimeError("å½“å‰ç¯å¢ƒä¸æ”¯æŒ BertLMHeadModelï¼Œè¯·å‡çº§ transformers æˆ–æ”¹ç”¨ mlm_only/mlm_nsp") from e
        model = BertLMHeadModel.from_pretrained(name)
        # ç¡®ä¿ causalï¼šæŸäº›ç‰ˆæœ¬éœ€ is_decoder=True
        if hasattr(model.config, "is_decoder"):
            model.config.is_decoder = True
        kind = "ltr"
    else:
        raise ValueError(f"Unknown ablation objective: {ablation}")
    return model, kind

def build_classification_model(num_labels: int, cfg: Dict[str, Any], use_bilstm: bool = False):
    """åˆ†ç±»æ¨¡å‹ï¼šå¤ç”¨ BERT ä¸»ä½“ + å¯é€‰ BiLSTM å¤´ï¼ˆä¹Ÿå¯ç›´æ¥ç”¨ BertForSequenceClassificationï¼‰ã€‚"""
    name = cfg["MODEL"]["name"]
    base = BertModel.from_pretrained(name)
    head = ClassificationHead(base.config.hidden_size, num_labels, use_bilstm=use_bilstm)
    return base, head

def build_qa_model(cfg: Dict[str, Any], use_bilstm: bool = False):
    """SQuAD æ¨¡å‹ï¼šBERT ä¸»ä½“ + SpanHeadï¼ˆæˆ–ç›´æ¥ç”¨ BertForQuestionAnsweringï¼‰ã€‚"""
    name = cfg["MODEL"]["name"]
    base = BertModel.from_pretrained(name)
    head = SpanHead(base.config.hidden_size, use_bilstm=use_bilstm)
    return base, head
```

23) `bert_ablation_factory/trainer/optimizer.py`
------------------------------------------------

```python
from __future__ import annotations
from typing import Dict, Any, Iterable
import torch
from torch.optim import AdamW

def build_optimizer(params: Iterable, cfg: Dict[str, Any]) -> torch.optim.Optimizer:
    lr = float(cfg["OPTIM"]["lr"])
    wd = float(cfg["OPTIM"]["weight_decay"])
    betas = tuple(cfg["OPTIM"].get("betas", [0.9, 0.999]))
    eps = float(cfg["OPTIM"].get("eps", 1e-8))
    return AdamW(params, lr=lr, betas=betas, eps=eps, weight_decay=wd)
```

24) `bert_ablation_factory/trainer/schedulers.py`
-------------------------------------------------

```python
from __future__ import annotations
from typing import Dict, Any
from torch.optim import Optimizer
from transformers import get_linear_schedule_with_warmup

def build_warmup_linear(optimizer: Optimizer, cfg: Dict[str, Any], num_training_steps: int):
    warmup_steps = int(cfg["OPTIM"]["warmup_steps"])
    return get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)
```

25) `bert_ablation_factory/trainer/eval.py`
-------------------------------------------

```python
from __future__ import annotations
from typing import Dict, Any
import torch

def accuracy(pred: torch.Tensor, target: torch.Tensor) -> float:
    """åˆ†ç±»å‡†ç¡®ç‡ã€‚"""
    pred_label = pred.argmax(dim=-1)
    correct = (pred_label == target).sum().item()
    return correct / max(1, target.numel())

def squad_em_f1(pred_start: torch.Tensor, pred_end: torch.Tensor,
                gold_start: torch.Tensor, gold_end: torch.Tensor) -> Dict[str, float]:
    """ç®€åŒ–ç‰ˆ EM/F1ï¼šä¸¥æ ¼åŒ¹é…ä¸ token çº§ F1ï¼ˆè¿‘ä¼¼ï¼‰ã€‚"""
    em = ((pred_start == gold_start) & (pred_end == gold_end)).float().mean().item()
    # token çº§é‡å 
    f1s = []
    for ps, pe, gs, ge in zip(pred_start.tolist(), pred_end.tolist(), gold_start.tolist(), gold_end.tolist()):
        pset = set(range(ps, pe + 1))
        gset = set(range(gs, ge + 1))
        inter = len(pset & gset)
        if len(pset) == 0 or len(gset) == 0:
            f1s.append(0.0)
        else:
            prec = inter / max(1, len(pset))
            rec = inter / max(1, len(gset))
            f1s.append(0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec))
    return {"em": em, "f1": sum(f1s) / max(1, len(f1s))}
```

26) `bert_ablation_factory/trainer/engine.py`
---------------------------------------------

```python
from __future__ import annotations
from typing import Dict, Any, Callable
from loguru import logger
from pathlib import Path
import torch
from torch.utils.data import DataLoader
from ..utils.tb import create_tb_writer
from .optimizer import build_optimizer
from .schedulers import build_warmup_linear

def _to_device(batch: Dict[str, torch.Tensor], device: torch.device) -> Dict[str, torch.Tensor]:
    return {k: v.to(device) for k, v in batch.items()}

def train_loop(
    model: torch.nn.Module,
    collate_loss_fn: Callable[[Dict[str, torch.Tensor]], Dict[str, torch.Tensor] | torch.Tensor],
    train_loader: DataLoader,
    valid_loader: DataLoader | None,
    cfg: Dict[str, Any],
    out_dir: Path,
    max_steps: int,
) -> None:
    """é€šç”¨è®­ç»ƒå¾ªç¯ï¼ˆæ”¯æŒ AMP + TB + æ–­ç‚¹ä¿å­˜ï¼‰ã€‚"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    optim = build_optimizer(model.parameters(), cfg)
    sched = build_warmup_linear(optim, cfg, num_training_steps=max_steps)
    scaler = torch.cuda.amp.GradScaler(enabled=bool(cfg.get("FP16", True)))
    writer = create_tb_writer(out_dir, "tb")

    step = 0
    model.train()
    for epoch in range(10_000_000):
        for batch in train_loader:
            step += 1
            batch = _to_device(batch, device)
            with torch.cuda.amp.autocast(enabled=bool(cfg.get("FP16", True))):
                outputs = collate_loss_fn(batch)
                loss = outputs["loss"] if isinstance(outputs, dict) else outputs

            scaler.scale(loss).backward()
            if step % int(cfg.get("GRAD_ACCUM_STEPS", 1)) == 0:
                scaler.step(optim)
                scaler.update()
                optim.zero_grad(set_to_none=True)
                sched.step()

            if step % int(cfg.get("LOG_EVERY", 50)) == 0:
                logger.info(f"step={step} loss={float(loss):.4f}")
                writer.add_scalar("train/loss", float(loss), step)

            if valid_loader is not None and step % int(cfg.get("EVAL_EVERY", 1000)) == 0:
                evaluate(model, valid_loader, collate_loss_fn, device, writer, step)

            if step % int(cfg.get("SAVE_EVERY", 1000)) == 0:
                save_path = out_dir / f"checkpoint_step_{step}.pt"
                torch.save({"model": model.state_dict(), "step": step}, save_path)
                logger.info(f"Saved checkpoint to {save_path}")

            if step >= max_steps:
                logger.info("Training finished.")
                return

@torch.no_grad()
def evaluate(model: torch.nn.Module, loader: DataLoader,
             collate_loss_fn: Callable[[Dict[str, torch.Tensor]], Dict[str, torch.Tensor] | torch.Tensor],
             device: torch.device, writer, step: int) -> None:
    model.eval()
    losses = []
    for batch in loader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = collate_loss_fn(batch)
        loss = outputs["loss"] if isinstance(outputs, dict) else outputs
        losses.append(float(loss))
    avg = sum(losses) / max(1, len(losses))
    writer.add_scalar("valid/loss", avg, step)
    model.train()
```

27) `bert_ablation_factory/cli/pretrain.py`
-------------------------------------------

```python
from __future__ import annotations
import argparse
from pathlib import Path
from typing import Dict, Any
from loguru import logger
from datasets import load_dataset, interleave_datasets
from torch.utils.data import DataLoader
import torch
from ..utils.io import load_yaml, merge_dict
from ..utils.logging import setup_logger
from ..utils.seed import fix_seed
from ..data.tokenization import build_tokenizer
from ..data.collators import MLMConfig, MLMNSPCollator, MLMOnlyCollator, LTRCollator
from ..modeling.build import build_pretrain_model
from ..trainer.engine import train_loop

def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser("BERT Ablation Pretrain")
    p.add_argument("--cfg", type=str, required=True, help="YAML é…ç½®")
    return p.parse_args()

def build_books_wiki_stream(tokenizer, max_len: int):
    """ä½¿ç”¨ bookcorpusopen + wikipedia ç®€å•æ‹¼æ¥ï¼Œæµå¼è¯»å–ï¼Œæ„é€  NSP å¥å¯¹ã€‚"""
    ds1 = load_dataset("bookcorpusopen", split="train", streaming=True)
    ds2 = load_dataset("wikipedia", "20220301.en", split="train", streaming=True)
    mixed = interleave_datasets([ds1, ds2], probabilities=[0.5, 0.5], seed=42)
    # å°†æ¯è¡Œ text åˆ†å¥ï¼ˆç²—ç•¥ï¼‰ï¼Œæ‹¼æˆå¥å¯¹æˆ–å•å¥
    sep = tokenizer.sep_token or "[SEP]"

    def gen_examples():
        prev = None
        for ex in mixed:
            text = (ex.get("text") or "").strip()
            if not text:
                continue
            sents = [s.strip() for s in text.split(".") if s.strip()]
            for s in sents:
                if prev is None:
                    prev = s
                    continue
                # 50% æ­£ä¾‹ï¼ˆä¸‹ä¸€å¥ï¼‰ï¼Œ50% è´Ÿä¾‹ï¼ˆéšæœºï¼‰
                import random
                if random.random() < 0.5:
                    a, b, label = prev, s, 0  # IsNext
                else:
                    b = sents[random.randrange(len(sents))]
                    a, label = prev, 1       # NotNext
                enc = tokenizer(
                    a, b,
                    truncation=True,
                    max_length=max_len,
                    padding="max_length",
                    return_token_type_ids=True,
                    return_attention_mask=True,
                )
                enc["next_sentence_label"] = label
                yield enc
                prev = s

    return gen_examples()

def main() -> None:
    args = parse_args()
    cfg = load_yaml(args.cfg)
    base_path = cfg.get("_base_")
    if base_path:
        base = load_yaml((Path(args.cfg).parent / base_path).resolve())
        cfg = merge_dict(base, {k: v for k, v in cfg.items() if k != "_base_"})
    setup_logger()
    fix_seed(int(cfg.get("SEED", 42)))

    out_dir = Path(cfg.get("OUTPUT_DIR", "runs")) / "pretrain"
    out_dir.mkdir(parents=True, exist_ok=True)

    tokenizer = build_tokenizer(cfg)
    model, kind = build_pretrain_model(cfg, cfg["ABLATION"]["objective"])
    model.train()

    # æ•°æ®
    max_len = int(cfg["DATA"].get("max_seq_len") or cfg["DATASET"].get("max_seq_len", 128))
    stream = build_books_wiki_stream(tokenizer, max_len)
    # ä¸ºäº† DataLoaderï¼Œéœ€è¦æŠŠæµå˜æˆæœ‰é™ iterableï¼ˆç¤ºä¾‹ï¼šæ¯æ­¥ä»æµä¸­æ‹‰æ ·æœ¬ï¼‰
    from itertools import islice

    def take(n):
        for item in islice(stream, n):
            yield item

    # collator
    if kind == "mlm_nsp":
        collator = MLMNSPCollator(MLMConfig(mask_strategy=cfg["ABLATION"]["mask_strategy"],
                                            pad_token_id=tokenizer.pad_token_id,
                                            mask_token_id=tokenizer.mask_token_id))
    elif kind == "mlm_only":
        collator = MLMOnlyCollator(MLMConfig(mask_strategy=cfg["ABLATION"]["mask_strategy"],
                                             pad_token_id=tokenizer.pad_token_id,
                                             mask_token_id=tokenizer.mask_token_id))
    else:
        collator = LTRCollator()

    # ç®€æ˜“ DataLoaderï¼ˆæ¯è½®ä»æµé‡ŒæŠ“å›ºå®šæ­¥æ•°ï¼‰
    per_device_bs = int(cfg["TRAIN"]["per_device_batch_size"])
    steps = int(cfg["TRAIN"]["max_steps"])
    eval_every = int(cfg["TRAIN"].get("eval_steps", 1000))
    save_every = int(cfg["TRAIN"].get("save_steps", 1000))

    class StreamDataset(torch.utils.data.IterableDataset):
        def __iter__(self):
            return take(steps * per_device_bs)

    train_loader = DataLoader(StreamDataset(), batch_size=per_device_bs, collate_fn=collator, num_workers=0)

    # æŸå¤±å°è£…ï¼šç›´æ¥ç”¨ HF æ¨¡å‹è‡ªå¸¦ lossï¼ˆForPreTraining/ForMaskedLM/LMHead éƒ½è¿”å› lossï¼‰
    def step_fn(batch):
        outputs = model(**batch)
        return {"loss": outputs.loss}

    logger.info(f"Start pretraining: ablation={kind}")
    train_loop(model, step_fn, train_loader, None, cfg, out_dir, max_steps=steps)

if __name__ == "__main__":
    main()
```

28) `bert_ablation_factory/cli/finetune_classification.py`
----------------------------------------------------------

```python
from __future__ import annotations
import argparse
from pathlib import Path
from typing import Any, Dict
from loguru import logger
import torch
from torch.utils.data import DataLoader
from transformers import BertForSequenceClassification
from ..utils.io import load_yaml, merge_dict
from ..utils.logging import setup_logger
from ..utils.seed import fix_seed
from ..data.tokenization import build_tokenizer
from ..data.glue import load_sst2
from ..trainer.optimizer import build_optimizer
from ..trainer.schedulers import build_warmup_linear
from ..trainer.eval import accuracy
from ..utils.tb import create_tb_writer

def parse_args():
    p = argparse.ArgumentParser("BERT Finetune - SST-2")
    p.add_argument("--cfg", type=str, required=True)
    return p.parse_args()

def main() -> None:
    args = parse_args()
    cfg = load_yaml(args.cfg)
    base_path = cfg.get("_base_")
    if base_path:
        base = load_yaml((Path(args.cfg).parent / base_path).resolve())
        cfg = merge_dict(base, {k: v for k, v in cfg.items() if k != "_base_"})

    setup_logger()
    fix_seed(int(cfg.get("SEED", 42)))
    out_dir = Path(cfg.get("OUTPUT_DIR", "runs")) / "sst2"
    out_dir.mkdir(parents=True, exist_ok=True)
    tb = create_tb_writer(out_dir, "tb")

    tokenizer = build_tokenizer(cfg)
    train_ds, dev_ds = load_sst2(tokenizer, int(cfg["DATA"]["max_seq_len"]))

    model = BertForSequenceClassification.from_pretrained(cfg["MODEL"]["name"], num_labels=2)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    train_loader = DataLoader(train_ds, batch_size=int(cfg["TRAIN"]["per_device_batch_size"]), shuffle=True)
    dev_loader = DataLoader(dev_ds, batch_size=64)

    optim = build_optimizer(model.parameters(), cfg)
    steps_per_epoch = len(train_loader)
    num_epochs = int(cfg["TRAIN"]["num_epochs"])
    max_steps = steps_per_epoch * num_epochs
    sched = build_warmup_linear(optim, cfg, max_steps)

    scaler = torch.cuda.amp.GradScaler(enabled=bool(cfg.get("FP16", True)))
    step = 0
    model.train()
    for epoch in range(num_epochs):
        for batch in train_loader:
            step += 1
            batch = {k: v.to(device) for k, v in batch.items()}
            with torch.cuda.amp.autocast(enabled=bool(cfg.get("FP16", True))):
                out = model(**batch)
                loss = out.loss
            scaler.scale(loss).backward()
            scaler.step(optim); scaler.update(); optim.zero_grad(set_to_none=True); sched.step()
            if step % int(cfg.get("LOG_EVERY", 50)) == 0:
                tb.add_scalar("train/loss", float(loss), step)
                logger.info(f"epoch={epoch} step={step} loss={float(loss):.4f}")

        # éªŒè¯
        model.eval()
        preds, gts = [], []
        with torch.no_grad():
            for vb in dev_loader:
                vb = {k: v.to(device) for k, v in vb.items()}
                logits = model(**vb).logits
                preds.append(logits.cpu())
                gts.append(vb["labels"].cpu())
        acc = accuracy(torch.cat(preds, 0), torch.cat(gts, 0))
        tb.add_scalar("valid/acc", acc, step)
        logger.info(f"[DEV] epoch={epoch} acc={acc:.4f}")
        model.train()

    torch.save(model.state_dict(), out_dir / "bert_sst2.pt")
    logger.info(f"Saved weights to {out_dir / 'bert_sst2.pt'}")

if __name__ == "__main__":
    main()
```

29) `bert_ablation_factory/cli/finetune_qa.py`
----------------------------------------------

```python
from __future__ import annotations
import argparse
from pathlib import Path
from typing import Any, Dict
from loguru import logger
import torch
from torch.utils.data import DataLoader
from transformers import BertModel
from ..utils.io import load_yaml, merge_dict
from ..utils.logging import setup_logger
from ..utils.seed import fix_seed
from ..data.tokenization import build_tokenizer
from ..data.squad import load_squad_v1
from ..modeling.build import build_qa_model
from ..trainer.optimizer import build_optimizer
from ..trainer.schedulers import build_warmup_linear
from ..trainer.eval import squad_em_f1
from ..utils.tb import create_tb_writer

def parse_args():
    p = argparse.ArgumentParser("BERT Finetune - SQuAD v1.1")
    p.add_argument("--cfg", type=str, required=True)
    return p.parse_args()

def main() -> None:
    args = parse_args()
    cfg = load_yaml(args.cfg)
    base_path = cfg.get("_base_")
    if base_path:
        base = load_yaml((Path(args.cfg).parent / base_path).resolve())
        cfg = merge_dict(base, {k: v for k, v in cfg.items() if k != "_base_"})

    setup_logger()
    fix_seed(int(cfg.get("SEED", 42)))
    out_dir = Path(cfg.get("OUTPUT_DIR", "runs")) / "squad_v1"
    out_dir.mkdir(parents=True, exist_ok=True)
    tb = create_tb_writer(out_dir, "tb")

    tokenizer = build_tokenizer(cfg)
    train_ds, dev_ds = load_squad_v1(
        tokenizer,
        int(cfg["DATA"]["max_seq_len"]),
        int(cfg["DATA"]["doc_stride"]),
        int(cfg["DATA"]["max_query_len"]),
    )

    base, head = build_qa_model(cfg, use_bilstm=bool(cfg["ABLATION"].get("use_bilstm_head", False)))
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    base.to(device); head.to(device)

    train_loader = DataLoader(train_ds, batch_size=int(cfg["TRAIN"]["per_device_batch_size"]), shuffle=True)
    dev_loader = DataLoader(dev_ds, batch_size=16)

    params = list(base.parameters()) + list(head.parameters())
    optim = build_optimizer(params, cfg)
    steps_per_epoch = len(train_loader)
    num_epochs = int(cfg["TRAIN"]["num_epochs"])
    max_steps = steps_per_epoch * num_epochs
    sched = build_warmup_linear(optim, cfg, max_steps)
    scaler = torch.cuda.amp.GradScaler(enabled=bool(cfg.get("FP16", True)))

    step = 0
    base.train(); head.train()
    for epoch in range(num_epochs):
        for batch in train_loader:
            step += 1
            batch = {k: v.to(device) for k, v in batch.items()}
            with torch.cuda.amp.autocast(enabled=bool(cfg.get("FP16", True))):
                out = base(input_ids=batch["input_ids"], attention_mask=batch["attention_mask"])
                start_logits, end_logits = head(out.last_hidden_state, batch["attention_mask"])
                loss_f = torch.nn.CrossEntropyLoss()
                loss = (loss_f(start_logits, batch["start_positions"])
                        + loss_f(end_logits, batch["end_positions"])) / 2.0
            scaler.scale(loss).backward()
            scaler.step(optim); scaler.update(); optim.zero_grad(set_to_none=True); sched.step()

            if step % int(cfg.get("LOG_EVERY", 50)) == 0:
                tb.add_scalar("train/loss", float(loss), step)
                logger.info(f"epoch={epoch} step={step} loss={float(loss):.4f}")

        # éªŒè¯
        base.eval(); head.eval()
        preds_s, preds_e = [], []
        gts_s, gts_e = [], []
        with torch.no_grad():
            for vb in dev_loader:
                vb = {k: v.to(device) for k, v in vb.items()}
                hs = base(input_ids=vb["input_ids"], attention_mask=vb["attention_mask"]).last_hidden_state
                s_log, e_log = head(hs, vb["attention_mask"])
                preds_s.append(s_log.argmax(-1).cpu()); preds_e.append(e_log.argmax(-1).cpu())
                gts_s.append(vb["start_positions"].cpu()); gts_e.append(vb["end_positions"].cpu())
        metric = squad_em_f1(torch.cat(preds_s), torch.cat(preds_e), torch.cat(gts_s), torch.cat(gts_e))
        tb.add_scalar("valid/em", metric["em"], step)
        tb.add_scalar("valid/f1", metric["f1"], step)
        logger.info(f"[DEV] epoch={epoch} EM={metric['em']:.4f} F1={metric['f1']:.4f}")
        base.train(); head.train()

    torch.save({"base": base.state_dict(), "head": head.state_dict()}, out_dir / "bert_squad_v1.pt")
    logger.info(f"Saved weights to {out_dir / 'bert_squad_v1.pt'}")

if __name__ == "__main__":
    main()
```

* * *

ä¸‰ã€ä½¿ç”¨ä¸æ‰©å±•å»ºè®®
=========

*   **å¼€å¯/å…³é—­ NSP**ï¼šåˆ‡æ¢ `configs/pretrain/mlm_nsp_base.yaml` â†” `mlm_no_nsp_base.yaml`ã€‚
*   **æ©ç ç­–ç•¥**ï¼š`ABLATION.mask_strategy` åœ¨ `80_10_10` ä¸ `100_mask` ä¹‹é—´åˆ‡æ¢ï¼Œè§‚å¯Ÿä¸‹æ¸¸å½±å“ã€‚
*   **BiLSTM å¤´**ï¼šåœ¨åˆ†ç±»æˆ– QA çš„ YAML ä¸­åŠ å…¥ `ABLATION.use_bilstm_head: true`ã€‚
*   **LTRï¼ˆå•å‘ï¼‰**ï¼šä½¿ç”¨ `configs/pretrain/ltr_no_nsp_base.yaml`ï¼›è‹¥ä½ æœ¬åœ° `transformers` ç‰ˆæœ¬æ—  `BertLMHeadModel` ä¼šæ”¶åˆ°æ¸…æ™°æŠ¥é”™æç¤ºï¼ˆæŒ‰è®ºæ–‡æ¶ˆèéœ€è¦ï¼Œå¯å‡çº§è‡³è¾ƒæ–°ç‰ˆæœ¬ï¼‰ã€‚
*   **é•¿åºåˆ—ï¼ˆ512ï¼‰**ï¼šæŠŠ `DATA.max_seq_len` ä» 128 æå‡åˆ° 512ï¼Œå¹¶ç›¸åº”é™ä½ batch æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ã€‚

å¦‚æœä½ æƒ³ç»§ç»­å®Œå–„ï¼š

1.  æˆ‘å¯ä»¥å†è¡¥ **NSP æ„é€ æ›´çœŸå®çš„æ­£è´Ÿä¾‹**ï¼ˆåŸºäºæ–‡æ¡£è¾¹ç•Œè€Œéè¡Œï¼‰ï¼Œ
2.  è¡¥å…… **GLUE å…¨å¥—ä»»åŠ¡** çš„ `TASKS` ä¸æ•°æ®ç®¡çº¿ï¼Œ
3.  å¢åŠ  **æ£€æŸ¥ç‚¹è‡ªåŠ¨é‡å¯ä¸å¤šéšæœºé‡å¯** å·¥å…·ï¼ˆå¤ç°å®éªŒæ–¹å·®ï¼‰ï¼Œ
4.  å¢åŠ  **æ··åˆæ•°æ®åŠ è½½ï¼ˆBooksCorpusOpen+Wiki çš„æ›´ä¸¥æ ¼å¥å­è¾¹ç•Œï¼‰** ä¸ **æµå¼è¯„ä¼°**ã€‚

éœ€è¦æˆ‘å†åŠ å“ªå—ï¼Œç›´æ¥è¯´ï¼Œæˆ‘ç»§ç»­æŠŠæ¨¡å—è¡¥é½ã€‚

## Prompt:
2025-09-17 23:11:06

2,3, please

## Response:
2025-09-17 23:12:54

å·²æ€è€ƒ 1m 42s

ç”¨æˆ·æƒ³è¦â€œ2,3â€ï¼Œå³å®ç°GLUEå®Œæ•´ä»»åŠ¡å’Œæ•°æ®ç®¡é“ï¼Œä»¥åŠåŠ å…¥æ£€æŸ¥ç‚¹è‡ªåŠ¨æ¢å¤å’Œå¤šé‡éšæœºé‡å¯ã€‚æˆ‘éœ€è¦æ›´æ–°ä»£ç ï¼Œé¦–å…ˆåœ¨data/glue.pyé‡Œå¢åŠ å¯¹å¤šä¸ªGLUEä»»åŠ¡çš„æ”¯æŒï¼Œå¹¶åŠ å…¥æ³¨å†ŒåŠŸèƒ½ã€‚è¿™å¯èƒ½æ¶‰åŠå¤šä¸ªæ–‡ä»¶çš„æ›´æ–°ï¼Œæˆ‘å°†ç¡®ä¿ä»£ç é£æ ¼ä¸€è‡´ï¼Œé¿å…ä½¿ç”¨åå°ä»»åŠ¡ã€‚

æˆ‘å°†æ›´æ–°é¡¹ç›®ç»“æ„ï¼Œç¡®ä¿æ•°æ®ç®¡é“æ”¯æŒå¤šä¸ªGLUEä»»åŠ¡ï¼Œå¹¶å°†è¿™äº›ä»»åŠ¡æ³¨å†Œåˆ°registry.pyçš„â€œTASKSâ€ä¸­ã€‚æ­¤å¤–ï¼Œè¿˜éœ€å®ç°å‘½ä»¤è¡Œæ¥å£ï¼Œæ”¯æŒè°ƒæ•´éšæœºé‡å¯æ¬¡æ•°ã€é€‰æ‹©æœ€ä½³éªŒè¯é›†ã€è‡ªåŠ¨æ¢å¤æ£€æŸ¥ç‚¹ç­‰åŠŸèƒ½ã€‚è¿˜è¦å¤„ç†checkpointä¿å­˜å’Œæ¢å¤åŠŸèƒ½ï¼Œä¸ç”¨æ¶‰åŠç½‘ç»œæµè§ˆã€‚

æˆ‘éœ€è¦æ·»åŠ ä¸€äº›åŠŸèƒ½ï¼Œé¦–å…ˆæä¾›æ–‡æœ¬æ ‡è®°å’Œæ ‡ç­¾æ˜ å°„ï¼Œè¿˜éœ€ä¸ºä»»åŠ¡æä¾›å®šåˆ¶çš„æŒ‡æ ‡ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä¼šæ›´æ–°é¡¹ç›®ç»“æ„ï¼Œæ·»åŠ æ–°çš„â€œtasks/glue.pyâ€æ¨¡å—ï¼Œå¤„ç†GLUEä»»åŠ¡æ³¨å†Œã€‚å¦å¤–ï¼Œè¿˜å°†å®ç°æ£€æŸ¥ç‚¹ä¿å­˜ä¸åŠ è½½åŠŸèƒ½ï¼Œå¹¶åœ¨å‘½ä»¤è¡Œå·¥å…·ä¸­æ·»åŠ è‡ªå®šä¹‰ä»»åŠ¡å’Œé‡å¯æ”¯æŒã€‚

æˆ‘å°†å®ç°è‡ªåŠ¨æ¢å¤åŠŸèƒ½ï¼Œç¡®ä¿èƒ½å¤ŸæŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹å¹¶æ¢å¤è®­ç»ƒçŠ¶æ€ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’Œç¼©æ”¾å™¨ã€‚æ­¤å¤–ï¼Œè¿˜ä¼šåœ¨`trainer/eval.py`é‡Œæ·»åŠ è¯„ä¼°æŒ‡æ ‡å‡½æ•°ï¼šè®¡ç®—CoLAçš„Matthewsç›¸å…³ç³»æ•°ï¼ŒQQP/MRPCçš„F1åˆ†æ•°ï¼ŒSTS-Bçš„Pearsonä¸Spearmanç›¸å…³ç³»æ•°ï¼Œå¹¶ä½¿ç”¨`evaluate`åº“æ¥ç®€åŒ–å®ç°ã€‚

å®Œæˆ

æ²¡é—®é¢˜ï¼ä½ è¦çš„ **2ï¼‰GLUE å…¨å¥—ä»»åŠ¡ç®¡çº¿** + **3ï¼‰æ–­ç‚¹è‡ªåŠ¨ç»­è®­ & å¤šéšæœºé‡å¯ï¼ˆmulti-restartï¼‰** æˆ‘å·²ç»è¡¥é½ã€‚ä¸‹é¢ç»™å‡º**å˜æ›´ç‚¹æ€»è§ˆ**ä¸**é€æ–‡ä»¶å®Œæ•´ä»£ç **ï¼ˆæ–°å¢/ä¿®æ”¹ï¼‰ã€‚ç›´æ¥æŒ‰é¡ºåºè¦†ç›–å³å¯è¿è¡Œã€‚

* * *

âœ… æ›´æ–°æ¦‚è§ˆï¼ˆNew / Changedï¼‰
=====================

```
bert-ablation-factory/
â”œâ”€ configs/
â”‚  â””â”€ finetune/
â”‚     â””â”€ glue_sst2_base.yaml        # CHANGED: å¢åŠ  RESTARTS/RESUMEï¼Œå¯åˆ‡æ¢ä»»æ„ GLUE ä»»åŠ¡
â”œâ”€ bert_ablation_factory/
â”‚  â”œâ”€ tasks/                        # NEW: ä»»åŠ¡å±‚ï¼ˆç»Ÿä¸€ GLUE ä»»åŠ¡æ³¨å†Œ/è¯„æµ‹ï¼‰
â”‚  â”‚  â”œâ”€ __init__.py                # NEW
â”‚  â”‚  â””â”€ glue.py                    # NEW: GLUE å…¨å¥—ï¼ˆSST-2/MNLI/QNLI/QQP/MRPC/RTE/CoLA/STS-Bï¼‰
â”‚  â”œâ”€ trainer/
â”‚  â”‚  â””â”€ checkpoint.py              # NEW: è‡ªåŠ¨æ‰¾æœ€è¿‘æ–­ç‚¹ï¼Œå®‰å…¨ä¿å­˜/æ¢å¤ä¼˜åŒ–å™¨/è°ƒåº¦å™¨/AMP
â”‚  â””â”€ cli/
â”‚     â””â”€ finetune_classification.py # CHANGED: æ”¯æŒä»»æ„ GLUE + å¤šé‡å¯é€‰æ‹© dev æœ€ä¼˜ + è‡ªåŠ¨ç»­è®­
```

* * *

1) `configs/finetune/glue_sst2_base.yaml`ï¼ˆ**ä¿®æ”¹**ï¼‰
-------------------------------------------------

> ç°åœ¨å¯ä»¥æŠŠ `TASK.name` æ”¹æˆ `glue_mnli/glue_qnli/glue_qqp/glue_mrpc/glue_rte/glue_cola/glue_stsb` ç­‰ã€‚  
> æ–°å¢ `RESTARTS`ï¼ˆå¤šéšæœºé‡å¯æ¬¡æ•°ï¼‰ä¸ `RESUME`ï¼ˆæ˜¯å¦è‡ªåŠ¨ç»­è®­ï¼‰ã€‚

```yaml
_base_: ../base.yaml

TASK:
  # æ”¯æŒï¼šglue_sst2, glue_mnli, glue_qnli, glue_qqp, glue_mrpc, glue_rte, glue_cola, glue_stsb
  name: glue_sst2

MODEL:
  name: bert-base-uncased
  revision: main

DATA:
  max_seq_len: 128

TRAIN:
  per_device_batch_size: 32
  num_epochs: 3
  eval_steps: 200         # æ¯ N step åœ¨ dev ä¸Šè¯„ä¼°ä¸€æ¬¡ï¼ˆå¯é€‰ï¼‰
  save_steps: 1000
  RESTARTS: 3             # å¤šéšæœºé‡å¯æ¬¡æ•°ï¼ˆå– dev ä¸»æŒ‡æ ‡æœ€ä¼˜çš„ runï¼‰
  RESUME: true            # æ–­ç‚¹è‡ªåŠ¨ç»­è®­

OPTIM:
  lr: 5.0e-5
  weight_decay: 0.01
  warmup_steps: 500
```

* * *

2) `bert_ablation_factory/tasks/__init__.py`ï¼ˆ**æ–°å¢**ï¼‰
----------------------------------------------------

```python
from __future__ import annotations
# ç©ºæ–‡ä»¶ç”¨äºæ ‡è®° tasks åŒ…
```

* * *

3) `bert_ablation_factory/tasks/glue.py`ï¼ˆ**æ–°å¢**ï¼‰
------------------------------------------------

```python
from __future__ import annotations
from typing import Dict, Any, Tuple, Optional, List
from datasets import load_dataset
from transformers import PreTrainedTokenizerBase
import evaluate
import numpy as np

from ..registry import TASKS

# GLUE ä¸»æŒ‡æ ‡æ˜ å°„ï¼ˆé€‰æ‹©å•ä¸€ä¸»æŒ‡æ ‡ç”¨äº early-stop/é€‰æœ€ä¼˜ runï¼‰
# å‚è€ƒ GLUE å®˜æ–¹ï¼šCoLA->matthews_correlationï¼›STS-B->pearson/spearmanï¼Œå¸¸ä»¥ pearson ä¸ºä¸»
MAIN_METRIC = {
    "sst2": "accuracy",
    "mnli": "accuracy",              # æˆ‘ä»¬ä½¿ç”¨ matched çš„ accuracy ä½œä¸ºä¸»æŒ‡æ ‡
    "qnli": "accuracy",
    "qqp": "f1",                     # åŒæ—¶ä¹Ÿä¼šå›æŠ¥ accuracy
    "mrpc": "f1",                    # åŒæ—¶ä¹Ÿä¼šå›æŠ¥ accuracy
    "rte": "accuracy",
    "cola": "matthews_correlation",
    "stsb": "pearson",
}

def _sentence_keys(task: str) -> Tuple[str, Optional[str]]:
    """è¿”å›ä¸åŒ GLUE ä»»åŠ¡çš„å¥å­åˆ—åï¼ˆå•å¥æˆ–å¥å¯¹ï¼‰ã€‚"""
    if task in ("sst2", "cola"):
        return "sentence", None
    if task in ("qnli", "rte"):
        return "question", "sentence"
    if task in ("mrpc", "qqp"):
        return "sentence1", "sentence2"
    if task in ("mnli",):
        return "premise", "hypothesis"
    if task in ("stsb",):
        return "sentence1", "sentence2"
    raise ValueError(f"Unsupported GLUE task: {task}")

def _num_labels_and_type(task: str) -> Tuple[int, str]:
    """è¿”å› (num_labels, problem_type)ã€‚STS-B æ˜¯å›å½’ï¼Œå…¶ä½™æ˜¯åˆ†ç±»ã€‚"""
    if task == "stsb":
        return 1, "regression"
    # å…¶ä½™åˆ†ç±»ä»»åŠ¡çš„ label æ•°
    num = {
        "sst2": 2, "cola": 2, "mrpc": 2, "qqp": 2, "qnli": 2, "rte": 2,
        "mnli": 3,  # entailment/neutral/contradiction
    }[task]
    return num, "single_label_classification"

def _metric_for(task: str):
    """åŠ è½½å¯¹åº”çš„ GLUE åº¦é‡å™¨ï¼ˆevaluateï¼‰ã€‚"""
    return evaluate.load("glue", task)

def _preprocess_builder(tokenizer: PreTrainedTokenizerBase, task: str, max_len: int):
    s1, s2 = _sentence_keys(task)

    def fn(batch):
        if s2 is None:
            enc = tokenizer(batch[s1], truncation=True, max_length=max_len, padding="max_length")
        else:
            enc = tokenizer(batch[s1], batch[s2], truncation=True, max_length=max_len, padding="max_length")
        # label ç›´æ¥ä¿ç•™
        enc["labels"] = batch["label"]
        return enc

    return fn

def _split_names(task: str) -> Tuple[str, Optional[str]]:
    """è¿”å› dev split åç§°ï¼ˆMNLI æœ‰ matched/mismatchedï¼‰ã€‚"""
    if task == "mnli":
        return "validation_matched", "validation_mismatched"
    return "validation", None

def _postprocess_logits(task: str, logits) -> np.ndarray:
    """å°†æ¨¡å‹è¾“å‡ºè½¬ä¸º metric éœ€è¦çš„é¢„æµ‹å€¼ï¼ˆåˆ†ç±»å– argmaxï¼Œå›å½’ç›´æ¥å–å€¼ï¼‰ã€‚"""
    if task == "stsb":
        return logits.squeeze(-1)
    return logits.argmax(axis=-1)

@TASKS.register("glue_sst2")
@TASKS.register("glue_mnli")
@TASKS.register("glue_qnli")
@TASKS.register("glue_qqp")
@TASKS.register("glue_mrpc")
@TASKS.register("glue_rte")
@TASKS.register("glue_cola")
@TASKS.register("glue_stsb")
def build_glue_task(cfg: Dict[str, Any], tokenizer: PreTrainedTokenizerBase):
    """ç»Ÿä¸€æ„å»º GLUE ä»»åŠ¡çš„æ•°æ®ä¸è¯„æµ‹ã€‚

    è¿”å›:
        {
          "task_name": str,                    # çº¯ task åï¼ˆä¸å« glue_ å‰ç¼€ï¼‰
          "train_ds": Dataset,
          "dev_ds": Dataset or dict[str, Dataset],  # MNLI æƒ…å†µä¸‹æœ‰ä¸¤ä¸ª dev
          "num_labels": int,
          "problem_type": "regression" | "single_label_classification",
          "metric": evaluate.Metric,
          "main_metric": str,                  # ä¸»æŒ‡æ ‡ key
        }
    """
    full = cfg["TASK"]["name"]          # e.g., "glue_sst2"
    assert full.startswith("glue_"), "TASK.name å¿…é¡»ä»¥ glue_ å¼€å¤´"
    task = full.split("_", 1)[1]

    raw = load_dataset("glue", task)
    max_len = int(cfg["DATA"]["max_seq_len"])
    fn = _preprocess_builder(tokenizer, task, max_len)

    train = raw["train"].map(fn, batched=True, remove_columns=raw["train"].column_names)
    train.set_format(type="torch")

    dev_name, dev_name2 = _split_names(task)
    dev = raw[dev_name].map(fn, batched=True, remove_columns=raw[dev_name].column_names)
    dev.set_format(type="torch")

    dev_bundle = dev
    if dev_name2 is not None:
        dev2 = raw[dev_name2].map(fn, batched=True, remove_columns=raw[dev_name2].column_names)
        dev2.set_format(type="torch")
        dev_bundle = {"matched": dev, "mismatched": dev2}

    num_labels, problem_type = _num_labels_and_type(task)
    metric = _metric_for(task)
    main_metric = MAIN_METRIC[task]

    return {
        "task_name": task,
        "train_ds": train,
        "dev_ds": dev_bundle,
        "num_labels": num_labels,
        "problem_type": problem_type,
        "metric": metric,
        "main_metric": main_metric,
    }

def compute_glue_metrics(task: str, metric, logits, labels) -> Dict[str, float]:
    """å¯¹å•ä¸€ split è®¡ç®— GLUE æŒ‡æ ‡ï¼ˆåˆ†ç±»å›å½’å…¼å®¹ï¼‰ã€‚"""
    logits = np.asarray(logits)
    labels = np.asarray(labels)
    preds = _postprocess_logits(task, logits)
    res = metric.compute(predictions=preds, references=labels)
    return {k: float(v) for k, v in res.items()}

def pick_main_score(task: str, metric_result: Dict[str, float]) -> float:
    """æŠ½å–ä¸»æŒ‡æ ‡åˆ†æ•°ï¼Œç”¨äº early-stop æˆ–å¤šé‡å¯é€‰æœ€ä¼˜ã€‚"""
    key = MAIN_METRIC[task]
    return float(metric_result.get(key, -1e9))
```

* * *

4) `bert_ablation_factory/trainer/checkpoint.py`ï¼ˆ**æ–°å¢**ï¼‰
--------------------------------------------------------

```python
from __future__ import annotations
from pathlib import Path
from typing import Any, Dict, Optional, Tuple
import torch
import re

def atomic_save(state: Dict[str, Any], path: Path) -> None:
    """åŸå­ä¿å­˜ï¼Œé¿å…ä¸­é€”ä¸­æ–­å¯¼è‡´åæ–‡ä»¶ã€‚"""
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = path.with_suffix(path.suffix + ".tmp")
    torch.save(state, tmp)
    tmp.replace(path)

def find_latest_checkpoint(dir_: Path, prefix: str = "ckpt_epoch_", suffix: str = ".pt") -> Optional[Path]:
    """åœ¨ç›®å½•ä¸‹å¯»æ‰¾æœ€è¿‘çš„æ–­ç‚¹æ–‡ä»¶ï¼Œå‘½åå¦‚ ckpt_epoch_0003.ptã€‚"""
    if not dir_.exists():
        return None
    pat = re.compile(rf"^{re.escape(prefix)}(\d+){re.escape(suffix)}$")
    best_n, best_path = -1, None
    for p in dir_.iterdir():
        if p.is_file():
            m = pat.match(p.name)
            if m:
                n = int(m.group(1))
                if n > best_n:
                    best_n, best_path = n, p
    return best_path

def save_checkpoint(
    path: Path,
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    scheduler: Any,
    scaler: Any,
    epoch: int,
    step: int,
) -> None:
    """ä¿å­˜æ¨¡å‹ä¸ä¼˜åŒ–å™¨/è°ƒåº¦å™¨/AMP çš„è®­ç»ƒçŠ¶æ€ã€‚"""
    state = {
        "model": model.state_dict(),
        "optimizer": optimizer.state_dict(),
        "scheduler": getattr(scheduler, "state_dict", lambda: {})(),
        "scaler": getattr(scaler, "state_dict", lambda: {})(),
        "epoch": epoch,
        "step": step,
        "rng": {
            "torch": torch.get_rng_state(),
            "cuda": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else [],
        },
    }
    atomic_save(state, path)

def load_checkpoint(
    path: Path,
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    scheduler: Any,
    scaler: Any,
) -> Tuple[int, int]:
    """åŠ è½½è®­ç»ƒçŠ¶æ€ï¼Œè¿”å› (epoch, step) ä»¥ä¾¿æ¢å¤å¾ªç¯æŒ‡é’ˆã€‚"""
    state = torch.load(path, map_location="cpu")
    model.load_state_dict(state["model"])
    optimizer.load_state_dict(state["optimizer"])
    try:
        scheduler.load_state_dict(state.get("scheduler", {}))
    except Exception:
        pass
    try:
        scaler.load_state_dict(state.get("scaler", {}))
    except Exception:
        pass
    rng = state.get("rng", {})
    torch.set_rng_state(rng.get("torch", torch.get_rng_state()))
    if torch.cuda.is_available() and rng.get("cuda"):
        torch.cuda.set_rng_state_all(rng["cuda"])
    return int(state.get("epoch", 0)), int(state.get("step", 0))
```

* * *

5) `bert_ablation_factory/cli/finetune_classification.py`ï¼ˆ**ä¿®æ”¹ï¼šGLUE å…¨å¥— + å¤šé‡å¯ + è‡ªåŠ¨ç»­è®­**ï¼‰
--------------------------------------------------------------------------------------

```python
from __future__ import annotations
import argparse
from pathlib import Path
from typing import Any, Dict, Tuple, List
from loguru import logger
import torch
from torch.utils.data import DataLoader
from transformers import BertForSequenceClassification
from ..utils.io import load_yaml, merge_dict
from ..utils.logging import setup_logger
from ..utils.seed import fix_seed
from ..data.tokenization import build_tokenizer
from ..trainer.optimizer import build_optimizer
from ..trainer.schedulers import build_warmup_linear
from ..utils.tb import create_tb_writer
from ..registry import TASKS
from ..tasks.glue import build_glue_task, compute_glue_metrics, pick_main_score
from ..trainer.checkpoint import save_checkpoint, load_checkpoint, find_latest_checkpoint

def parse_args():
    p = argparse.ArgumentParser("BERT Finetune - GLUE (Classification/Regression)")
    p.add_argument("--cfg", type=str, required=True)
    p.add_argument("--restarts", type=int, default=None, help="è¦†ç›– YAML çš„ RESTARTS")
    p.add_argument("--resume", action="store_true", help="è¦†ç›– YAML çš„ RESUME=True")
    return p.parse_args()

def _build_model(cfg: Dict[str, Any], num_labels: int, problem_type: str) -> BertForSequenceClassification:
    model = BertForSequenceClassification.from_pretrained(cfg["MODEL"]["name"], num_labels=num_labels)
    if problem_type == "regression":
        model.config.problem_type = "regression"
    return model

def _evaluate_once(
    model: BertForSequenceClassification,
    device: torch.device,
    loader: DataLoader,
    task: str,
    metric_obj,
) -> Dict[str, float]:
    model.eval()
    logits_all, labels_all = [], []
    with torch.no_grad():
        for batch in loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            out = model(**batch)
            logits_all.append(out.logits.detach().cpu().numpy())
            labels_all.append(batch["labels"].detach().cpu().numpy())
    import numpy as np
    logits = np.concatenate(logits_all, axis=0)
    labels = np.concatenate(labels_all, axis=0)
    return compute_glue_metrics(task, metric_obj, logits, labels)

def run_single_restart(
    cfg: Dict[str, Any],
    task_bundle: Dict[str, Any],
    run_dir: Path,
    seed: int,
) -> Tuple[float, Dict[str, float]]:
    """å•æ¬¡é‡å¯è®­ç»ƒå¹¶åœ¨ dev ä¸Šè¯„ä¼°ï¼Œè¿”å› (ä¸»æŒ‡æ ‡, æŒ‡æ ‡å­—å…¸)ã€‚"""
    fix_seed(seed)
    tb = create_tb_writer(run_dir, "tb")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # æ•°æ®
    train_ds = task_bundle["train_ds"]
    dev_ds = task_bundle["dev_ds"]
    task = task_bundle["task_name"]
    metric = task_bundle["metric"]
    main_key = task_bundle["main_metric"]

    model = _build_model(cfg, task_bundle["num_labels"], task_bundle["problem_type"]).to(device)
    optim = build_optimizer(model.parameters(), cfg)

    steps_per_epoch = (len(train_ds) // int(cfg["TRAIN"]["per_device_batch_size"])) + 1
    num_epochs = int(cfg["TRAIN"]["num_epochs"])
    max_steps = steps_per_epoch * num_epochs
    sched = build_warmup_linear(optim, cfg, max_steps)
    scaler = torch.cuda.amp.GradScaler(enabled=bool(cfg.get("FP16", True)))

    # Loader
    train_loader = DataLoader(train_ds, batch_size=int(cfg["TRAIN"]["per_device_batch_size"]), shuffle=True)
    dev_loaders: Dict[str, DataLoader] | DataLoader
    if isinstance(dev_ds, dict):
        dev_loaders = {k: DataLoader(v, batch_size=64) for k, v in dev_ds.items()}
    else:
        dev_loaders = DataLoader(dev_ds, batch_size=64)

    # è‡ªåŠ¨ç»­è®­
    start_epoch, global_step = 0, 0
    latest = find_latest_checkpoint(run_dir)
    want_resume = bool(cfg["TRAIN"].get("RESUME", False))
    if want_resume and latest is not None:
        logger.info(f"[RESUME] Found checkpoint: {latest}")
        start_epoch, global_step = load_checkpoint(latest, model, optim, sched, scaler)
        logger.info(f"[RESUME] epoch={start_epoch}, step={global_step}")

    # è®­ç»ƒå¾ªç¯
    model.train()
    best_score, best_metrics = -1e9, {}
    for epoch in range(start_epoch, num_epochs):
        for batch_idx, batch in enumerate(train_loader):
            global_step += 1
            batch = {k: v.to(device) for k, v in batch.items()}
            with torch.cuda.amp.autocast(enabled=bool(cfg.get("FP16", True))):
                out = model(**batch)
                loss = out.loss
            scaler.scale(loss).backward()
            scaler.step(optim); scaler.update(); optim.zero_grad(set_to_none=True); sched.step()

            if global_step % int(cfg.get("LOG_EVERY", 50)) == 0:
                tb.add_scalar("train/loss", float(loss), global_step)

            # ä¸­é€”è¯„ä¼°
            if global_step % int(cfg["TRAIN"].get("eval_steps", 0) or 0) == 0:
                if isinstance(dev_loaders, dict):
                    all_metrics = {}
                    for k, ld in dev_loaders.items():
                        res = _evaluate_once(model, device, ld, task, metric)
                        all_metrics.update({f"{k}/{kk}": vv for kk, vv in res.items()})
                    main = all_metrics.get(f"matched/{main_key}", None)
                    if main is None:  # è‹¥ä¸»æŒ‡æ ‡ä¸åœ¨ matched ä¸Šï¼Œå°±é€€åŒ–ä¸ºä»»ä¸€å¯ç”¨çš„ä¸»æŒ‡æ ‡
                        # ä¾‹å¦‚å¦‚æœä½ æƒ³ä»¥å¹³å‡ä¸ºä¸»æŒ‡æ ‡ï¼Œå¯ä»¥è‡ªè¡Œæ”¹é€ 
                        main = max([v for k, v in all_metrics.items() if k.endswith(main_key)])
                    score = float(main)
                    for k, v in all_metrics.items():
                        tb.add_scalar(f"dev/{k}", v, global_step)
                else:
                    res = _evaluate_once(model, device, dev_loaders, task, metric)
                    score = pick_main_score(task, res)
                    for k, v in res.items():
                        tb.add_scalar(f"dev/{k}", v, global_step)

                if score > best_score:
                    best_score, best_metrics = score, res if not isinstance(dev_loaders, dict) else all_metrics
                    # ä¿å­˜å½“å‰æœ€ä¼˜
                    save_checkpoint(run_dir / "best.pt", model, optim, sched, scaler, epoch, global_step)

        # æ¯ä¸ª epoch æœ«å°¾ä¿å­˜æ–­ç‚¹
        ckpt_path = run_dir / f"ckpt_epoch_{epoch:04d}.pt"
        save_checkpoint(ckpt_path, model, optim, sched, scaler, epoch, global_step)

    return float(best_score), {k: float(v) for k, v in best_metrics.items()}

def main() -> None:
    args = parse_args()
    cfg = load_yaml(args.cfg)
    base_path = cfg.get("_base_")
    if base_path:
        base = load_yaml((Path(args.cfg).parent / base_path).resolve())
        cfg = merge_dict(base, {k: v for k, v in cfg.items() if k != "_base_"})

    # è¦†ç›– YAML
    if args.restarts is not None:
        cfg["TRAIN"]["RESTARTS"] = int(args.restarts)
    if args.resume:
        cfg["TRAIN"]["RESUME"] = True

    setup_logger()

    # æ„é€ ä»»åŠ¡
    from ..tasks.glue import build_glue_task  # ä¿æŒæ˜¾å¼ä¾èµ–
    tokenizer = build_tokenizer(cfg)
    bundle = build_glue_task(cfg, tokenizer)
    task = bundle["task_name"]

    # å¤šéšæœºé‡å¯ï¼ˆä¸åŒ seedï¼‰
    base_seed = int(cfg.get("SEED", 42))
    restarts = int(cfg["TRAIN"].get("RESTARTS", 1))
    root_out = Path(cfg.get("OUTPUT_DIR", "runs")) / f"{bundle['task_name']}"
    root_out.mkdir(parents=True, exist_ok=True)

    best_overall, best_detail, best_run = -1e9, {}, -1
    for r in range(restarts):
        run_dir = root_out / f"run_{r:02d}"
        run_dir.mkdir(parents=True, exist_ok=True)
        score, detail = run_single_restart(cfg, bundle, run_dir, seed=base_seed + r)
        logger.info(f"[RUN {r}] main={score:.4f} metrics={detail}")
        if score > best_overall:
            best_overall, best_detail, best_run = score, detail, r

    logger.info(f"[BEST] run={best_run} main={best_overall:.4f} metrics={best_detail}")
    # å°†æœ€ä½³ run çš„ best.pt å¤åˆ¶åˆ°æ ¹ç›®å½•ï¼ˆä¾›éƒ¨ç½²/è¯„æµ‹ï¼‰
    import shutil
    src = (root_out / f"run_{best_run:02d}" / "best.pt")
    if src.exists():
        shutil.copy2(src, root_out / "best.pt")
        logger.info(f"Saved overall best checkpoint to {root_out / 'best.pt'}")

if __name__ == "__main__":
    main()
```

* * *

ä½¿ç”¨æ–¹æ³•
====

*   **åˆ‡æ¢ä»»åŠ¡**ï¼šåœ¨ `configs/finetune/glue_sst2_base.yaml` é‡ŒæŠŠ `TASK.name` æ”¹æˆï¼š  
    `glue_sst2 / glue_mnli / glue_qnli / glue_qqp / glue_mrpc / glue_rte / glue_cola / glue_stsb`
*   **å¤šéšæœºé‡å¯**ï¼š
    *   YAML è®¾ `TRAIN.RESTARTS: 3`ï¼Œæˆ–å‘½ä»¤è¡Œ `--restarts 5` è¦†ç›–ã€‚
    *   ç¨‹åºä¼šåœ¨ `runs/<task>/run_00, run_01, ...` ä¸‹åˆ†åˆ«è®­ç»ƒï¼Œ**è‡ªåŠ¨é€‰æ‹© dev ä¸»æŒ‡æ ‡æœ€ä¼˜**çš„ runï¼Œå¹¶æŠŠ `best.pt` æå‡åˆ° `runs/<task>/best.pt`ã€‚
*   **æ–­ç‚¹è‡ªåŠ¨ç»­è®­**ï¼š
    *   YAML è®¾ `TRAIN.RESUME: true`ï¼Œæˆ–å‘½ä»¤è¡Œ `--resume`ã€‚
    *   æ¯ä¸ª epoch åœ¨ `ckpt_epoch_XXXX.pt` ä¿å­˜ï¼›å†æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä»è¯¥ run ç›®å½•ä¸‹**æœ€è¿‘æ–­ç‚¹**æ¢å¤ï¼ˆå«ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ä¸ AMP çŠ¶æ€ï¼‰ã€‚

* * *

å°ç»“ä¸åç»­å¯é€‰å¢å¼º
=========

*   ç°åœ¨ä½ å·²ç»æ‹¥æœ‰ï¼š**GLUE å…¨å®¶æ¡¶**çš„ç»Ÿä¸€ä»»åŠ¡æ„å»ºä¸**å¯å®¡è®¡**çš„ä¸»æŒ‡æ ‡é€‰æ‹©é€»è¾‘ï¼›
*   **å·¥ä¸šåŒ–è®­ç»ƒæ¢å¤**ä¸**å¤šé‡å¯**æµç¨‹ï¼Œå®Œå…¨è´´åˆè®ºæ–‡â€œBERT-Large å°æ•°æ®éœ€å¤šæ¬¡éšæœºé‡å¯â€çš„å®è·µå»ºè®®ã€‚
*   å¦‚æœéœ€è¦ï¼Œæˆ‘å¯ä»¥ç»§ç»­æŠŠ **MNLIï¼ˆmatched/mismatchedï¼‰ä¸»æŒ‡æ ‡**æ”¹ä¸ºâ€œäºŒè€…å¹³å‡â€ï¼Œæˆ–ä¸º **QQP/MRPC** è‡ªå®šä¹‰â€œF1 ä¸»æŒ‡æ ‡ + Acc ç›‘æ§â€çš„â€œåŒé˜ˆå€¼æ—©åœâ€ï¼Œä¹Ÿå¯ä»¥åŠ ä¸€ä¸ª **YAML ç½‘æ ¼æœç´¢å™¨**ï¼ˆåœ¨ lr/batch/epoch ä¸Šè‡ªåŠ¨æ‰«å‚å¹¶äº§å‡ºè¡¨æ ¼ï¼‰ã€‚



---
Powered by [ChatGPT Exporter](https://www.chatgptexporter.com)